{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb3fc4c",
   "metadata": {},
   "source": [
    "# ğŸš€ EntraÃ®nement ICM sur Google Colab\n",
    "\n",
    "Ce notebook entraÃ®ne un agent DQN avec **CuriositÃ© (ICM)** pour la navigation robotique.\n",
    "\n",
    "## ğŸ¯ Configuration:\n",
    "- GPU: T4/V100 (gratuit)\n",
    "- Ã‰pisodes: 1500\n",
    "- Environnement: NavigationEnv (800x600)\n",
    "- ICM: Feature + Inverse + Forward Models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ce7021",
   "metadata": {},
   "source": [
    "## ğŸ“¦ 1. Installation des DÃ©pendances\n",
    "\n",
    "â±ï¸ ~2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070aa7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision gymnasium pygame numpy matplotlib seaborn pyyaml tqdm scipy pandas tensorboard\n",
    "\n",
    "print(\"âœ… Installation terminÃ©e!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e72cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"ğŸ”¥ CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   MÃ©moire: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"   âš ï¸  CPU mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0788e",
   "metadata": {},
   "source": [
    "## ğŸ“ 2. Cloner le Projet depuis GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208d9b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/mohhajji-1111/projet_RL.git\n",
    "%cd projet_RL\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e203e6c4",
   "metadata": {},
   "source": [
    "## ğŸ”§ 3. Configuration Python Path\n",
    "\n",
    "âš ï¸ **IMPORTANT**: Cette cellule configure le path pour que les imports fonctionnent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba29d5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurer le Python path\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"âœ… Project root: {project_root}\")\n",
    "print(f\"âœ… Python path configurÃ©\")\n",
    "\n",
    "# VÃ©rifier la structure\n",
    "print(\"\\nğŸ“ VÃ©rification des fichiers:\")\n",
    "if os.path.exists('src/agents/curiosity_agent.py'):\n",
    "    print(\"   âœ… curiosity_agent.py trouvÃ©\")\n",
    "if os.path.exists('configs/curiosity_config.yaml'):\n",
    "    print(\"   âœ… curiosity_config.yaml trouvÃ©\")\n",
    "if os.path.exists('src/environment/navigation_env.py'):\n",
    "    print(\"   âœ… navigation_env.py trouvÃ©\")\n",
    "\n",
    "print(\"\\nğŸ¯ PrÃªt pour l'entraÃ®nement!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db88414",
   "metadata": {},
   "source": [
    "## ğŸ® 4. Test de l'Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862aaf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.environment.navigation_env import NavigationEnv\n",
    "import numpy as np\n",
    "\n",
    "# CrÃ©er environnement\n",
    "env = NavigationEnv(width=800, height=600, render_mode=None)\n",
    "\n",
    "print(\"âœ… Environnement crÃ©Ã©!\")\n",
    "print(f\"   Observation space: {env.observation_space.shape}\")\n",
    "print(f\"   Action space: {env.action_space.n} actions\")\n",
    "\n",
    "# Test rapide\n",
    "state, _ = env.reset()\n",
    "print(f\"\\nğŸ“Š Test rapide (5 steps):\")\n",
    "for i in range(5):\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    print(f\"   Step {i+1}: action={action}, reward={reward:.2f}\")\n",
    "\n",
    "print(\"\\nâœ… Environnement OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649b2c30",
   "metadata": {},
   "source": [
    "## ğŸ§  5. CrÃ©er l'Agent avec CuriositÃ©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ebbcb",
   "metadata": {},
   "source": [
    "## âš ï¸ VÃ‰RIFICATION AVANT IMPORT\n",
    "\n",
    "**IMPORTANT:** Si tu obtiens une erreur `ModuleNotFoundError`, vÃ©rifie que:\n",
    "1. Tu as exÃ©cutÃ© la **Cellule 5** (Configuration Python Path)\n",
    "2. Tu es dans le bon rÃ©pertoire (`projet_RL/`)\n",
    "3. Le dossier `src/` existe\n",
    "\n",
    "ExÃ©cute la cellule ci-dessous pour vÃ©rifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d7d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"ğŸ” VÃ©rification de l'environnement...\\n\")\n",
    "\n",
    "# 1. RÃ©pertoire actuel\n",
    "current_dir = os.getcwd()\n",
    "print(f\"ğŸ“ RÃ©pertoire: {current_dir}\")\n",
    "\n",
    "# 2. VÃ©rifier la structure\n",
    "checks = {\n",
    "    'src/': os.path.exists('src'),\n",
    "    'src/agents/': os.path.exists('src/agents'),\n",
    "    'src/agents/curiosity_agent.py': os.path.exists('src/agents/curiosity_agent.py'),\n",
    "    'configs/curiosity_config.yaml': os.path.exists('configs/curiosity_config.yaml'),\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ“‹ Fichiers requis:\")\n",
    "for path, exists in checks.items():\n",
    "    status = \"âœ…\" if exists else \"âŒ\"\n",
    "    print(f\"   {status} {path}\")\n",
    "\n",
    "# 3. VÃ©rifier Python path\n",
    "print(f\"\\nğŸ Python path:\")\n",
    "project_in_path = any('projet_RL' in p or current_dir in p for p in sys.path)\n",
    "print(f\"   {'âœ…' if project_in_path else 'âŒ'} Project dans sys.path\")\n",
    "print(f\"   sys.path[0]: {sys.path[0]}\")\n",
    "\n",
    "# 4. Verdict\n",
    "if all(checks.values()) and project_in_path:\n",
    "    print(\"\\nâœ… TOUT EST OK! Tu peux importer CuriosityAgent\")\n",
    "else:\n",
    "    print(\"\\nâŒ PROBLÃˆME DÃ‰TECTÃ‰!\")\n",
    "    if not all(checks.values()):\n",
    "        print(\"   â†’ ExÃ©cute la Cellule 4 (git clone)\")\n",
    "    if not project_in_path:\n",
    "        print(\"   â†’ ExÃ©cute la Cellule 5 (Configuration Python Path)\")\n",
    "    print(\"\\nğŸ’¡ Puis rÃ©exÃ©cute cette cellule de vÃ©rification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531d5820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agents.curiosity_agent import CuriosityAgent\n",
    "import yaml\n",
    "\n",
    "# Charger config\n",
    "with open('configs/curiosity_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# CrÃ©er agent\n",
    "agent = CuriosityAgent(\n",
    "    state_dim=env.observation_space.shape[0],\n",
    "    action_dim=env.action_space.n,\n",
    "    config=config['agent'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"âœ… CuriosityAgent crÃ©Ã©!\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   State dim: {env.observation_space.shape[0]}\")\n",
    "print(f\"   Action dim: {env.action_space.n}\")\n",
    "print(f\"   Feature dim: {config['agent']['feature_dim']}\")\n",
    "print(f\"   Curiosity beta: {config['agent']['curiosity_beta']}\")\n",
    "\n",
    "print(\"\\nğŸ“Š ParamÃ¨tres des rÃ©seaux:\")\n",
    "print(f\"   Feature Network: {sum(p.numel() for p in agent.feature_net.parameters()):,} params\")\n",
    "print(f\"   Inverse Model: {sum(p.numel() for p in agent.inverse_model.parameters()):,} params\")\n",
    "print(f\"   Forward Model: {sum(p.numel() for p in agent.forward_model.parameters()):,} params\")\n",
    "print(f\"   Q-Network: {sum(p.numel() for p in agent.q_network.parameters()):,} params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede0e14",
   "metadata": {},
   "source": [
    "## ğŸš€ 6. ENTRAÃNEMENT\n",
    "\n",
    "ğŸ¯ Lance l'entraÃ®nement avec suivi en temps rÃ©el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5693f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import deque\n",
    "\n",
    "# ParamÃ¨tres\n",
    "num_episodes = 1500\n",
    "eval_interval = 50\n",
    "save_interval = 100\n",
    "\n",
    "# CrÃ©er dossiers\n",
    "os.makedirs('results/models/curiosity', exist_ok=True)\n",
    "os.makedirs('results/plots', exist_ok=True)\n",
    "\n",
    "# MÃ©triques\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "intrinsic_rewards = []\n",
    "forward_losses = []\n",
    "inverse_losses = []\n",
    "success_rate = deque(maxlen=100)\n",
    "avg_rewards = deque(maxlen=100)\n",
    "best_reward = -float('inf')\n",
    "\n",
    "print(\"ğŸš€ DÃ©but de l'entraÃ®nement...\\n\")\n",
    "\n",
    "pbar = tqdm(range(num_episodes), desc=\"Training\")\n",
    "\n",
    "for episode in pbar:\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_intrinsic = 0\n",
    "    episode_length = 0\n",
    "    done = False\n",
    "    truncated = False\n",
    "    \n",
    "    while not (done or truncated):\n",
    "        # Action\n",
    "        action = agent.select_action(state)\n",
    "        \n",
    "        # Step\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        # RÃ©compense intrinsÃ¨que\n",
    "        intrinsic_reward = agent.compute_intrinsic_reward(\n",
    "            torch.FloatTensor(state).unsqueeze(0).to(device),\n",
    "            torch.LongTensor([action]).to(device),\n",
    "            torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "        )\n",
    "        \n",
    "        # Stocker\n",
    "        agent.store_transition(state, action, reward, next_state, done or truncated)\n",
    "        \n",
    "        # EntraÃ®ner\n",
    "        if len(agent.replay_buffer) > agent.batch_size:\n",
    "            loss = agent.train_step()\n",
    "        \n",
    "        episode_reward += reward\n",
    "        episode_intrinsic += intrinsic_reward\n",
    "        episode_length += 1\n",
    "        state = next_state\n",
    "    \n",
    "    # MÃ©triques\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_lengths.append(episode_length)\n",
    "    intrinsic_rewards.append(episode_intrinsic)\n",
    "    avg_rewards.append(episode_reward)\n",
    "    success_rate.append(1.0 if info.get('success', False) else 0.0)\n",
    "    \n",
    "    icm_stats = agent.get_icm_stats()\n",
    "    forward_losses.append(icm_stats['forward_loss'])\n",
    "    inverse_losses.append(icm_stats['inverse_loss'])\n",
    "    \n",
    "    # Progress\n",
    "    pbar.set_postfix({\n",
    "        'Reward': f\"{episode_reward:.2f}\",\n",
    "        'Avg': f\"{np.mean(avg_rewards):.2f}\",\n",
    "        'Success': f\"{np.mean(success_rate):.1%}\",\n",
    "        'Îµ': f\"{agent.epsilon:.3f}\"\n",
    "    })\n",
    "    \n",
    "    # Sauvegarder meilleur\n",
    "    if episode_reward > best_reward:\n",
    "        best_reward = episode_reward\n",
    "        agent.save_checkpoint('results/models/curiosity/best.pth')\n",
    "    \n",
    "    # Checkpoints\n",
    "    if (episode + 1) % save_interval == 0:\n",
    "        agent.save_checkpoint(f'results/models/curiosity/checkpoint_{episode+1}.pth')\n",
    "        print(f\"\\nğŸ’¾ Checkpoint: episode {episode+1}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… ENTRAÃNEMENT TERMINÃ‰!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Meilleure rÃ©compense: {best_reward:.2f}\")\n",
    "print(f\"Taux de succÃ¨s final: {np.mean(success_rate):.2%}\")\n",
    "print(f\"RÃ©compense moyenne (100 derniers): {np.mean(avg_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0fbb00",
   "metadata": {},
   "source": [
    "## ğŸ“Š 7. Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d0f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. RÃ©compenses\n",
    "ax = axes[0, 0]\n",
    "ax.plot(episode_rewards, alpha=0.3, color='blue')\n",
    "smoothed = gaussian_filter1d(episode_rewards, sigma=20)\n",
    "ax.plot(smoothed, color='red', linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('ğŸ“ˆ RÃ©compenses d\\'Ã‰pisode')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# 2. Moyenne mobile\n",
    "ax = axes[0, 1]\n",
    "window = 100\n",
    "moving_avg = [np.mean(episode_rewards[max(0, i-window):i+1]) for i in range(len(episode_rewards))]\n",
    "ax.plot(moving_avg, color='green', linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Avg Reward')\n",
    "ax.set_title(f'ğŸ“Š Moyenne Mobile (n={window})')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# 3. CuriositÃ©\n",
    "ax = axes[1, 0]\n",
    "smoothed_int = gaussian_filter1d(intrinsic_rewards, sigma=20)\n",
    "ax.plot(smoothed_int, color='orange', linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Intrinsic Reward')\n",
    "ax.set_title('ğŸ” RÃ©compenses IntrinsÃ¨ques')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# 4. ICM Losses\n",
    "ax = axes[1, 1]\n",
    "ax.plot(gaussian_filter1d(forward_losses, sigma=20), label='Forward', color='purple', linewidth=2)\n",
    "ax.plot(gaussian_filter1d(inverse_losses, sigma=20), label='Inverse', color='brown', linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('ğŸ§  ICM Losses')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# 5. Longueur\n",
    "ax = axes[2, 0]\n",
    "smoothed_len = gaussian_filter1d(episode_lengths, sigma=20)\n",
    "ax.plot(smoothed_len, color='darkblue', linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Steps')\n",
    "ax.set_title('â±ï¸ Longueur des Ã‰pisodes')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# 6. Distribution\n",
    "ax = axes[2, 1]\n",
    "ax.hist(episode_rewards, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "ax.axvline(np.mean(episode_rewards), color='red', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel('Reward')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('ğŸ“Š Distribution des RÃ©compenses')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/plots/training_summary.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Graphiques sauvegardÃ©s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0f353a",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ 8. Statistiques Finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bd7e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š STATISTIQUES FINALES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nğŸ¯ RÃ©compenses:\")\n",
    "print(f\"   Moyenne: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}\")\n",
    "print(f\"   MÃ©diane: {np.median(episode_rewards):.2f}\")\n",
    "print(f\"   Min/Max: {np.min(episode_rewards):.2f} / {np.max(episode_rewards):.2f}\")\n",
    "print(f\"   Meilleur: {best_reward:.2f}\")\n",
    "\n",
    "last_100 = episode_rewards[-100:]\n",
    "print(f\"\\nğŸ“Š Performance (100 derniers):\")\n",
    "print(f\"   Moyenne: {np.mean(last_100):.2f} Â± {np.std(last_100):.2f}\")\n",
    "print(f\"   SuccÃ¨s: {np.mean(list(success_rate)):.2%}\")\n",
    "\n",
    "print(f\"\\nâ±ï¸ Ã‰pisodes:\")\n",
    "print(f\"   Longueur moyenne: {np.mean(episode_lengths):.1f} steps\")\n",
    "\n",
    "print(f\"\\nğŸ” ICM:\")\n",
    "print(f\"   RÃ©compense intrinsÃ¨que moy: {np.mean(intrinsic_rewards):.4f}\")\n",
    "print(f\"   Forward loss finale: {forward_losses[-1]:.4f}\")\n",
    "print(f\"   Inverse loss finale: {inverse_losses[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ§  Agent:\")\n",
    "print(f\"   Epsilon final: {agent.epsilon:.4f}\")\n",
    "print(f\"   Buffer size: {len(agent.replay_buffer)}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4848a97",
   "metadata": {},
   "source": [
    "## ğŸ’¾ 9. TÃ©lÃ©charger les RÃ©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e3dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compresser\n",
    "!zip -r results.zip results/\n",
    "\n",
    "# TÃ©lÃ©charger\n",
    "from google.colab import files\n",
    "files.download('results.zip')\n",
    "\n",
    "print(\"âœ… results.zip tÃ©lÃ©chargÃ©!\")\n",
    "print(\"\\nContenu:\")\n",
    "print(\"  ğŸ“ models/ - ModÃ¨les entraÃ®nÃ©s (.pth)\")\n",
    "print(\"  ğŸ“ plots/ - Graphiques (.png)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed9668",
   "metadata": {},
   "source": [
    "## ğŸ‰ EntraÃ®nement TerminÃ©!\n",
    "\n",
    "### Fichiers gÃ©nÃ©rÃ©s:\n",
    "- âœ… `results/models/curiosity/best.pth` - Meilleur modÃ¨le\n",
    "- âœ… `results/plots/training_summary.png` - Visualisations\n",
    "\n",
    "### Prochaines Ã©tapes:\n",
    "1. TÃ©lÃ©charge `results.zip`\n",
    "2. Teste le modÃ¨le localement avec l'interface GUI\n",
    "3. Compare avec un agent DQN classique\n",
    "\n",
    "**Bon courage! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
