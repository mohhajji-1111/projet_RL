# ğŸ¤– Robot Navigation with Reinforcement Learning

A comprehensive reinforcement learning framework for training autonomous robot navigation using DQN and Rainbow DQN algorithms.

## ğŸ“‹ Table of Contents

- [Features](#features)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Usage](#usage)
- [Configuration](#configuration)
- [Training](#training)
- [Evaluation](#evaluation)
- [Visualization](#visualization)
- [Architecture](#architecture)
- [Contributing](#contributing)
- [License](#license)

## âœ¨ Features

### Agents
- **DQN (Deep Q-Network)**: Classic deep reinforcement learning algorithm
- **Rainbow DQN**: Advanced agent with:
  - Double DQN
  - Dueling Networks
  - Noisy Networks for exploration
  - Prioritized Experience Replay support

### Environment
- **2D Navigation Environment**: Gymnasium-compatible environment
- **Dynamic Obstacles**: Moving obstacles with configurable patterns
- **LIDAR Sensor**: Simulated laser range finder
- **Configurable Physics**: Adjustable robot dynamics and environment parameters

### Training Features
- **Basic Training**: Standard DQN training loop
- **Adaptive Training**: Curriculum learning with automatic difficulty adjustment
- **Prioritized Replay Buffer**: More efficient learning from important experiences
- **Comprehensive Logging**: TensorBoard integration and custom metrics tracking

### Visualization
- **Advanced Renderer**: Professional pygame-based visualization
- **Particle Effects**: Visual feedback for events (collisions, goal reached)
- **Training GUI**: Real-time training monitoring with live plots
- **Trajectory Tracking**: Visual path history

## ğŸ“ Project Structure

```
robot-navigation-rl/
â”œâ”€â”€ ğŸ“ src/
â”‚   â”œâ”€â”€ agents/              # RL agent implementations
â”‚   â”‚   â”œâ”€â”€ base_agent.py    # Abstract base class
â”‚   â”‚   â”œâ”€â”€ dqn_agent.py     # Basic DQN
â”‚   â”‚   â””â”€â”€ rainbow_agent.py # Rainbow DQN
â”‚   â”œâ”€â”€ environment/         # Environment components
â”‚   â”‚   â”œâ”€â”€ navigation_env.py
â”‚   â”‚   â”œâ”€â”€ obstacles.py
â”‚   â”‚   â””â”€â”€ sensors.py
â”‚   â”œâ”€â”€ training/            # Training utilities
â”‚   â”‚   â”œâ”€â”€ trainer_base.py
â”‚   â”‚   â”œâ”€â”€ train_basic.py
â”‚   â”‚   â””â”€â”€ train_adaptive.py
â”‚   â”œâ”€â”€ visualization/       # Rendering and GUI
â”‚   â”‚   â”œâ”€â”€ renderer.py
â”‚   â”‚   â”œâ”€â”€ effects.py
â”‚   â”‚   â””â”€â”€ gui.py
â”‚   â””â”€â”€ utils/               # Utility modules
â”‚       â”œâ”€â”€ replay_buffer.py
â”‚       â”œâ”€â”€ logger.py
â”‚       â””â”€â”€ metrics.py
â”‚
â”œâ”€â”€ ğŸ“ notebooks/            # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_environment_test.ipynb
â”‚   â”œâ”€â”€ 02_dqn_training.ipynb
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸ“ configs/              # Configuration files
â”‚   â”œâ”€â”€ base_config.yaml
â”‚   â”œâ”€â”€ rainbow_config.yaml
â”‚   â””â”€â”€ adaptive_config.yaml
â”‚
â”œâ”€â”€ ğŸ“ scripts/              # Executable scripts
â”‚   â”œâ”€â”€ train.py             # Main training
â”‚   â”œâ”€â”€ evaluate.py          # Model evaluation
â”‚   â”œâ”€â”€ generate_plots.py    # Visualization
â”‚   â””â”€â”€ demo.py              # Live demo
â”‚
â”œâ”€â”€ ğŸ“ trained_models/       # Saved models
â”œâ”€â”€ ğŸ“ results/              # Training results
â”‚   â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ videos/
â”‚   â””â”€â”€ logs/
â”‚
â”œâ”€â”€ ğŸ“ tests/                # Unit tests
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

## ğŸš€ Installation

### Prerequisites
- Python 3.8+
- CUDA-capable GPU (optional, for faster training)

### Install from source

```bash
# Clone the repository
git clone https://github.com/yourusername/robot-navigation-rl.git
cd robot-navigation-rl

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install package in development mode
pip install -e .
```

## ğŸ¯ Quick Start

### 1. Test Environment

```python
from src.environment import NavigationEnv

env = NavigationEnv(render_mode='human')
state, info = env.reset()

for _ in range(1000):
    action = env.action_space.sample()
    state, reward, terminated, truncated, info = env.step(action)
    if terminated or truncated:
        break

env.close()
```

### 2. Train DQN Agent

```bash
python scripts/train.py --config configs/base_config.yaml
```

### 3. Evaluate Trained Model

```bash
python scripts/evaluate.py --model trained_models/basic/final.pt --render
```

### 4. Run Live Demo

```bash
python scripts/demo.py --model trained_models/basic/final.pt --episodes 5
```

## ğŸ“– Usage

### Training

#### Basic Training
```bash
python scripts/train.py --config configs/base_config.yaml --seed 42
```

#### Rainbow Agent
```bash
python scripts/train.py --config configs/rainbow_config.yaml --device cuda
```

#### Adaptive Curriculum Learning
```bash
python scripts/train.py --config configs/adaptive_config.yaml
```

### Evaluation

```bash
# Evaluate with rendering
python scripts/evaluate.py --model trained_models/basic/final.pt \
    --agent-type dqn --episodes 20 --render

# Evaluate without rendering (faster)
python scripts/evaluate.py --model trained_models/rainbow/final.pt \
    --agent-type rainbow --episodes 100
```

### Generate Plots

```bash
python scripts/generate_plots.py \
    --log results/logs/experiment_metrics.json \
    --output results/figures
```

## âš™ï¸ Configuration

Configuration files use YAML format. Example:

```yaml
# configs/base_config.yaml
environment:
  width: 800
  height: 600
  robot_radius: 15.0
  max_speed: 5.0

agent:
  type: "dqn"
  learning_rate: 0.0001
  gamma: 0.99
  hidden_dims: [256, 256]

training:
  num_episodes: 1000
  batch_size: 64
  epsilon_start: 1.0
  epsilon_decay: 0.995
```

## ğŸ“ Training Details

### DQN Algorithm
- Experience replay buffer
- Target network with soft updates
- Epsilon-greedy exploration
- Huber loss for stability

### Rainbow DQN Improvements
- **Double DQN**: Reduces overestimation bias
- **Dueling Networks**: Separate value and advantage streams
- **Noisy Networks**: Learnable exploration
- **Prioritized Replay**: Focus on important transitions

### Curriculum Learning
The adaptive trainer implements curriculum learning:
1. **Easy Stage**: No obstacles, close goal
2. **Medium Stage**: Static obstacles
3. **Hard Stage**: Dynamic obstacles, complex scenarios

## ğŸ“Š Evaluation Metrics

The framework tracks:
- Episode rewards
- Episode lengths
- Success rate
- Training loss
- Q-value estimates
- TD errors
- Exploration rate

## ğŸ¨ Visualization

### Training Curves
Automatically generated plots include:
- Reward progression
- Episode lengths
- Loss curves
- Q-value evolution

### Live Demo
Interactive visualization with:
- Robot trajectory
- Particle effects
- Real-time statistics
- Goal indicators

## ğŸ—ï¸ Architecture

### Agent Architecture
```
Input (State)
    â†“
Feature Extraction (MLP)
    â†“
[Optional: Dueling Networks]
    â”œâ”€â†’ Value Stream
    â””â”€â†’ Advantage Stream
    â†“
Q-Values (Actions)
```

### Training Loop
```
1. Initialize agent, environment, replay buffer
2. For each episode:
   a. Reset environment
   b. Select action (Îµ-greedy or noisy)
   c. Execute action, observe reward
   d. Store transition in buffer
   e. Sample batch and train
   f. Update target network
3. Save model and metrics
```

## ğŸ§ª Testing

Run unit tests:
```bash
pytest tests/
```

## ğŸ“ Examples

### Custom Agent

```python
from src.agents import BaseAgent
import torch.nn as nn

class CustomAgent(BaseAgent):
    def __init__(self, state_dim, action_dim):
        super().__init__(state_dim, action_dim)
        # Your custom implementation
    
    def select_action(self, state, epsilon=0.0):
        # Your action selection logic
        pass
    
    def train_step(self, batch):
        # Your training logic
        pass
```

### Custom Environment

```python
from src.environment import NavigationEnv

class CustomEnv(NavigationEnv):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # Add your customizations
    
    def _calculate_reward(self, distance, collision, goal_reached):
        # Custom reward function
        return reward
```

## ğŸ¤ Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- OpenAI Gymnasium for the environment interface
- DeepMind for the DQN and Rainbow algorithms
- PyTorch team for the deep learning framework

## ğŸ“§ Contact

For questions or suggestions:
- Open an issue on GitHub
- Email: your.email@example.com

## ğŸ”— References

1. Mnih et al. (2015). "Human-level control through deep reinforcement learning"
2. Van Hasselt et al. (2016). "Deep Reinforcement Learning with Double Q-learning"
3. Wang et al. (2016). "Dueling Network Architectures for Deep Reinforcement Learning"
4. Hessel et al. (2018). "Rainbow: Combining Improvements in Deep Reinforcement Learning"

---

â­ **Star this repository if you find it helpful!**
