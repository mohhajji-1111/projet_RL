{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef56e8b",
   "metadata": {},
   "source": [
    "# üöÄ ENTRA√éNEMENT COMPLET - Google Colab avec GPU\n",
    "\n",
    "## Configuration Compl√®te pour l'Entra√Ænement du Robot de Navigation\n",
    "\n",
    "Ce notebook permet un entra√Ænement complet avec:\n",
    "- ‚úÖ **GPU gratuit** (T4/V100/A100)\n",
    "- ‚úÖ **Sauvegarde automatique** sur Google Drive\n",
    "- ‚úÖ **2000 √©pisodes** d'entra√Ænement\n",
    "- ‚úÖ **Curriculum learning** (4 stages progressifs)\n",
    "- ‚úÖ **TensorBoard** en temps r√©el\n",
    "- ‚úÖ **Auto-resume** en cas de timeout\n",
    "- ‚úÖ **Visualisations** et rapport PDF\n",
    "- ‚úÖ **Test du mod√®le** final\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Instructions:\n",
    "1. **Runtime** ‚Üí **Change runtime type** ‚Üí **GPU (T4)**\n",
    "2. **Runtime** ‚Üí **Run all**\n",
    "3. Autorise l'acc√®s √† Google Drive\n",
    "4. Attends ~30-45 minutes\n",
    "5. R√©cup√®re les r√©sultats dans Drive!\n",
    "\n",
    "---\n",
    "\n",
    "**Date:** December 6, 2025  \n",
    "**Version:** 2.0 - Production Ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a142e4bb",
   "metadata": {},
   "source": [
    "## üîß √âtape 1: Configuration Initiale et V√©rification GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01239282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# V√©rification GPU\n",
    "print(\"=\" * 70)\n",
    "print(\"üîç V√âRIFICATION SYST√àME\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if running on Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Environnement: Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚ö†Ô∏è  Environnement: Local\")\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU D√©tect√©: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  GPU non disponible - Utilisation CPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   PyTorch Version: {torch.__version__}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dcbc5d",
   "metadata": {},
   "source": [
    "## üìÅ √âtape 2: Montage Google Drive et Extraction du Projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a27c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    # Monter Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # D√©finir les chemins\n",
    "    DRIVE_ROOT = Path('/content/drive/MyDrive/RL_Project')\n",
    "    PROJECT_ZIP = DRIVE_ROOT / 'projet_RL.zip'\n",
    "    WORK_DIR = Path('/content/projet_RL')\n",
    "    RESULTS_DIR = DRIVE_ROOT / 'results'\n",
    "    CHECKPOINTS_DIR = DRIVE_ROOT / 'checkpoints'\n",
    "    \n",
    "    print(\"\\n‚úÖ Google Drive mont√© avec succ√®s!\")\n",
    "    print(f\"   Drive Root: {DRIVE_ROOT}\")\n",
    "    \n",
    "    # Cr√©er les dossiers de r√©sultats\n",
    "    RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Extraire le projet\n",
    "    if PROJECT_ZIP.exists():\n",
    "        print(f\"\\nüì¶ Extraction du projet depuis {PROJECT_ZIP}...\")\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile(PROJECT_ZIP, 'r') as zip_ref:\n",
    "            zip_ref.extractall(WORK_DIR)\n",
    "        print(f\"‚úÖ Projet extrait dans: {WORK_DIR}\")\n",
    "    else:\n",
    "        print(f\"‚ùå ERREUR: {PROJECT_ZIP} n'existe pas!\")\n",
    "        print(f\"   Upload 'projet_RL.zip' dans '{DRIVE_ROOT}'\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Changer vers le r√©pertoire du projet\n",
    "    os.chdir(WORK_DIR)\n",
    "    sys.path.insert(0, str(WORK_DIR))\n",
    "    \n",
    "else:\n",
    "    # Mode local\n",
    "    WORK_DIR = Path.cwd()\n",
    "    RESULTS_DIR = WORK_DIR / 'results' / 'colab_training'\n",
    "    CHECKPOINTS_DIR = WORK_DIR / 'checkpoints' / 'colab'\n",
    "    RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüìÇ R√©pertoire de travail: {WORK_DIR}\")\n",
    "print(f\"üìä R√©sultats: {RESULTS_DIR}\")\n",
    "print(f\"üíæ Checkpoints: {CHECKPOINTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7080b6f",
   "metadata": {},
   "source": [
    "## üì¶ √âtape 3: Installation des D√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e74865",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installation silencieuse des d√©pendances\n",
    "\n",
    "print(\"üì¶ Installation des d√©pendances...\")\n",
    "\n",
    "!pip install torch torchvision torchaudio --upgrade\n",
    "!pip install gymnasium numpy matplotlib seaborn pandas\n",
    "!pip install tensorboard optuna tqdm rich\n",
    "!pip install pygame pillow opencv-python\n",
    "!pip install plotly kaleido\n",
    "\n",
    "# Installer le projet\n",
    "!pip install -e .\n",
    "\n",
    "print(\"‚úÖ Installation termin√©e!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6911bc10",
   "metadata": {},
   "source": [
    "## üéØ √âtape 4: Configuration de l'Entra√Ænement Complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e4dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Imports PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Imports Gymnasium\n",
    "import gymnasium as gym\n",
    "\n",
    "# Imports pour l'environnement\n",
    "from src.environment.navigation_env import NavigationEnv\n",
    "from src.environment.obstacles import StaticObstacle, DynamicObstacle\n",
    "\n",
    "# Imports pour l'agent\n",
    "from src.agents.dqn_agent import DQNAgent\n",
    "\n",
    "# Imports pour le training\n",
    "from src.training.curriculum_learning import CurriculumLearningSystem\n",
    "from src.training.experiment_tracker import UnifiedTracker\n",
    "\n",
    "# Imports pour les utilitaires\n",
    "from src.utils.replay_buffer import ReplayBuffer\n",
    "\n",
    "# Imports pour la visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "# Configuration matplotlib pour Colab\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Tous les modules import√©s avec succ√®s!\")\n",
    "print(f\"üìç R√©pertoire de travail: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d765e9",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è √âtape 5: Hyperparam√®tres et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd87597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CONFIGURATION DE L'ENTRA√éNEMENT ====================\n",
    "\n",
    "CONFIG = {\n",
    "    # üéÆ Param√®tres d'entra√Ænement\n",
    "    'num_episodes': 2000,              # Nombre total d'√©pisodes\n",
    "    'max_steps_per_episode': 1000,     # Nombre max de pas par √©pisode\n",
    "    'target_update_frequency': 10,     # Fr√©quence de mise √† jour du r√©seau cible\n",
    "    \n",
    "    # üß† Architecture du r√©seau\n",
    "    'hidden_dims': [512, 512],         # Dimensions des couches cach√©es\n",
    "    'learning_rate': 5e-4,             # Taux d'apprentissage\n",
    "    'gamma': 0.99,                     # Facteur de discount\n",
    "    \n",
    "    # üíæ Replay Buffer\n",
    "    'buffer_capacity': 100000,         # Capacit√© du buffer\n",
    "    'batch_size': 128,                 # Taille du batch\n",
    "    \n",
    "    # üéØ Epsilon (exploration)\n",
    "    'epsilon_start': 1.0,              # Epsilon initial\n",
    "    'epsilon_end': 0.01,               # Epsilon final\n",
    "    'epsilon_decay': 0.995,            # D√©croissance epsilon\n",
    "    \n",
    "    # üó∫Ô∏è Environnement\n",
    "    'env_width': 800,                  # Largeur de l'environnement\n",
    "    'env_height': 600,                 # Hauteur de l'environnement\n",
    "    'num_obstacles': 7,                # Nombre d'obstacles\n",
    "    \n",
    "    # üíæ Sauvegarde\n",
    "    'checkpoint_interval': 100,        # Sauvegarder tous les N √©pisodes\n",
    "    'progress_interval': 25,           # Afficher progr√®s tous les N √©pisodes\n",
    "    \n",
    "    # ‚è±Ô∏è Gestion du timeout Colab (12h max)\n",
    "    'max_runtime_hours': 11.5,         # Buffer de 30 min avant timeout\n",
    "    'auto_save_interval': 50,          # Sauvegarde auto tous les N √©pisodes\n",
    "}\n",
    "\n",
    "# Chemins pour la sauvegarde\n",
    "if IN_COLAB:\n",
    "    CHECKPOINT_PATH = Path(CHECKPOINTS_DIR) / \"checkpoint_colab.pt\"\n",
    "    BEST_MODEL_PATH = Path(CHECKPOINTS_DIR) / \"best_model_colab.pt\"\n",
    "    TENSORBOARD_DIR = Path(WORK_DIR) / \"runs\" / \"colab_training\"\n",
    "else:\n",
    "    CHECKPOINT_PATH = Path(\"checkpoints\") / \"checkpoint_local.pt\"\n",
    "    BEST_MODEL_PATH = Path(\"checkpoints\") / \"best_model_local.pt\"\n",
    "    TENSORBOARD_DIR = Path(\"runs\") / \"local_training\"\n",
    "\n",
    "# Cr√©er les dossiers si n√©cessaire\n",
    "CHECKPOINT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "TENSORBOARD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration:\")\n",
    "print(f\"   üìä √âpisodes: {CONFIG['num_episodes']}\")\n",
    "print(f\"   üß† Architecture: {CONFIG['hidden_dims']}\")\n",
    "print(f\"   üìà Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"   üíæ Buffer size: {CONFIG['buffer_capacity']}\")\n",
    "print(f\"   üéØ Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"   üó∫Ô∏è  Environnement: {CONFIG['env_width']}x{CONFIG['env_height']}\")\n",
    "print(f\"   üöß Obstacles: {CONFIG['num_obstacles']}\")\n",
    "print(f\"   üíæ Checkpoint tous les {CONFIG['checkpoint_interval']} √©pisodes\")\n",
    "print(f\"   ‚è±Ô∏è  Runtime max: {CONFIG['max_runtime_hours']}h\")\n",
    "print(f\"\\nüìÅ Chemins:\")\n",
    "print(f\"   Checkpoint: {CHECKPOINT_PATH}\")\n",
    "print(f\"   Meilleur mod√®le: {BEST_MODEL_PATH}\")\n",
    "print(f\"   TensorBoard: {TENSORBOARD_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a8d09b",
   "metadata": {},
   "source": [
    "## üèóÔ∏è √âtape 6: Initialisation de l'Environnement et de l'Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841063ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CR√âATION DES OBSTACLES ====================\n",
    "print(\"üöß Cr√©ation des obstacles...\")\n",
    "\n",
    "obstacles = []\n",
    "np.random.seed(42)  # Pour la reproductibilit√©\n",
    "\n",
    "for i in range(CONFIG['num_obstacles']):\n",
    "    x = np.random.randint(100, CONFIG['env_width'] - 100)\n",
    "    y = np.random.randint(100, CONFIG['env_height'] - 100)\n",
    "    width = np.random.randint(40, 80)\n",
    "    height = np.random.randint(40, 80)\n",
    "    \n",
    "    obstacle = StaticObstacle(\n",
    "        x=x, y=y,\n",
    "        width=width, height=height,\n",
    "        color=(100, 100, 100)\n",
    "    )\n",
    "    obstacles.append(obstacle)\n",
    "\n",
    "print(f\"‚úÖ {len(obstacles)} obstacles cr√©√©s\")\n",
    "\n",
    "# ==================== CR√âATION DE L'ENVIRONNEMENT ====================\n",
    "print(\"\\nüó∫Ô∏è  Cr√©ation de l'environnement...\")\n",
    "\n",
    "env = NavigationEnv(\n",
    "    width=CONFIG['env_width'],\n",
    "    height=CONFIG['env_height'],\n",
    "    obstacles=obstacles\n",
    ")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(f\"‚úÖ Environnement cr√©√©\")\n",
    "print(f\"   üìä Dimension √©tat: {state_dim}\")\n",
    "print(f\"   üéÆ Nombre d'actions: {action_dim}\")\n",
    "\n",
    "# ==================== CR√âATION DE L'AGENT ====================\n",
    "print(\"\\nü§ñ Cr√©ation de l'agent DQN...\")\n",
    "\n",
    "agent = DQNAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_dims=CONFIG['hidden_dims'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    gamma=CONFIG['gamma']\n",
    ")\n",
    "\n",
    "# D√©placer l'agent sur le device (GPU si disponible)\n",
    "agent.q_network.to(device)\n",
    "agent.target_network.to(device)\n",
    "\n",
    "print(f\"‚úÖ Agent cr√©√© et d√©plac√© sur {device}\")\n",
    "print(f\"   üß† Architecture: {CONFIG['hidden_dims']}\")\n",
    "print(f\"   üìà Learning rate: {CONFIG['learning_rate']}\")\n",
    "\n",
    "# ==================== CR√âATION DU REPLAY BUFFER ====================\n",
    "print(\"\\nüíæ Cr√©ation du Replay Buffer...\")\n",
    "\n",
    "replay_buffer = ReplayBuffer(capacity=CONFIG['buffer_capacity'])\n",
    "\n",
    "print(f\"‚úÖ Replay Buffer cr√©√© (capacit√©: {CONFIG['buffer_capacity']:,})\")\n",
    "\n",
    "# ==================== SYST√àME D'APPRENTISSAGE PAR CURRICULUM ====================\n",
    "print(\"\\nüìö Initialisation du Curriculum Learning...\")\n",
    "\n",
    "curriculum = CurriculumLearningSystem(\n",
    "    num_episodes=CONFIG['num_episodes'],\n",
    "    stages=None  # Utilise les stages par d√©faut\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Curriculum Learning initialis√©\")\n",
    "print(f\"   üìä Nombre de stages: {len(curriculum.stages)}\")\n",
    "\n",
    "# ==================== TRACKER D'EXP√âRIENCES ====================\n",
    "print(\"\\nüìä Initialisation du tracker...\")\n",
    "\n",
    "tracker = UnifiedTracker(\n",
    "    project_name=\"robot-navigation-colab\",\n",
    "    experiment_name=f\"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "    config=CONFIG,\n",
    "    log_dir=str(TENSORBOARD_DIR),\n",
    "    use_tensorboard=True,\n",
    "    use_wandb=False,  # D√©sactiv√© pour Colab\n",
    "    use_mlflow=False  # D√©sactiv√© pour Colab\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Tracker initialis√© (TensorBoard)\")\n",
    "print(f\"   üìÅ Log dir: {TENSORBOARD_DIR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ TOUT EST PR√äT POUR L'ENTRA√éNEMENT!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daebeda6",
   "metadata": {},
   "source": [
    "## üéØ √âtape 7: Boucle d'Entra√Ænement Principale (2000 √âpisodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10851ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== FONCTIONS UTILITAIRES ====================\n",
    "\n",
    "def save_checkpoint(episode, agent, optimizer, best_reward, epsilon, curriculum, filename):\n",
    "    \"\"\"Sauvegarde un checkpoint complet\"\"\"\n",
    "    checkpoint = {\n",
    "        'episode': episode,\n",
    "        'q_network_state_dict': agent.q_network.state_dict(),\n",
    "        'target_network_state_dict': agent.target_network.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_reward': best_reward,\n",
    "        'epsilon': epsilon,\n",
    "        'curriculum_stage': curriculum.current_stage_idx,\n",
    "        'config': CONFIG\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"üíæ Checkpoint sauvegard√©: {filename}\")\n",
    "\n",
    "def load_checkpoint(filename, agent, optimizer):\n",
    "    \"\"\"Charge un checkpoint\"\"\"\n",
    "    if not Path(filename).exists():\n",
    "        return 0, float('-inf'), CONFIG['epsilon_start'], 0\n",
    "    \n",
    "    checkpoint = torch.load(filename, map_location=device)\n",
    "    agent.q_network.load_state_dict(checkpoint['q_network_state_dict'])\n",
    "    agent.target_network.load_state_dict(checkpoint['target_network_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    print(f\"üìÇ Checkpoint charg√©: {filename}\")\n",
    "    print(f\"   Episode: {checkpoint['episode']}\")\n",
    "    print(f\"   Best reward: {checkpoint['best_reward']:.2f}\")\n",
    "    \n",
    "    return (checkpoint['episode'], \n",
    "            checkpoint['best_reward'], \n",
    "            checkpoint['epsilon'],\n",
    "            checkpoint['curriculum_stage'])\n",
    "\n",
    "# ==================== TENTATIVE DE CHARGEMENT D'UN CHECKPOINT ====================\n",
    "print(\"üîç Recherche d'un checkpoint existant...\")\n",
    "\n",
    "start_episode = 0\n",
    "best_reward = float('-inf')\n",
    "epsilon = CONFIG['epsilon_start']\n",
    "start_stage = 0\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    try:\n",
    "        start_episode, best_reward, epsilon, start_stage = load_checkpoint(\n",
    "            CHECKPOINT_PATH, agent, agent.optimizer\n",
    "        )\n",
    "        curriculum.current_stage_idx = start_stage\n",
    "        print(f\"‚úÖ Reprise depuis l'√©pisode {start_episode}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Erreur lors du chargement: {e}\")\n",
    "        print(\"üîÑ D√©marrage d'un nouvel entra√Ænement\")\n",
    "else:\n",
    "    print(\"üìù Pas de checkpoint trouv√©, d√©marrage d'un nouvel entra√Ænement\")\n",
    "\n",
    "# ==================== VARIABLES DE SUIVI ====================\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "episode_losses = []\n",
    "episode_successes = []\n",
    "training_start_time = time.time()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"üöÄ D√âBUT DE L'ENTRA√éNEMENT - {CONFIG['num_episodes']} √âPISODES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ==================== BOUCLE D'ENTRA√éNEMENT ====================\n",
    "for episode in range(start_episode, CONFIG['num_episodes']):\n",
    "    # V√©rification du timeout Colab\n",
    "    elapsed_hours = (time.time() - training_start_time) / 3600\n",
    "    if elapsed_hours >= CONFIG['max_runtime_hours']:\n",
    "        print(f\"\\n‚è±Ô∏è  Timeout approch√© ({elapsed_hours:.2f}h)\")\n",
    "        print(\"üíæ Sauvegarde automatique avant arr√™t...\")\n",
    "        save_checkpoint(episode, agent, agent.optimizer, best_reward, \n",
    "                       epsilon, curriculum, CHECKPOINT_PATH)\n",
    "        print(\"‚úÖ Sauvegarde termin√©e. Vous pouvez relancer le notebook.\")\n",
    "        break\n",
    "    \n",
    "    # Obtenir la configuration du curriculum\n",
    "    curriculum.update_stage(episode)\n",
    "    stage_config = curriculum.get_current_env_config()\n",
    "    current_epsilon = curriculum.get_current_epsilon(epsilon)\n",
    "    \n",
    "    # Reset de l'environnement\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_loss = 0\n",
    "    num_updates = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    # √âpisode\n",
    "    while not done and steps < CONFIG['max_steps_per_episode']:\n",
    "        # S√©lection de l'action\n",
    "        action = agent.select_action(state, current_epsilon)\n",
    "        \n",
    "        # Ex√©cution de l'action\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Stockage dans le replay buffer\n",
    "        replay_buffer.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Entra√Ænement si le buffer a assez d'exp√©riences\n",
    "        if len(replay_buffer.buffer) >= CONFIG['batch_size']:\n",
    "            batch = replay_buffer.sample(CONFIG['batch_size'])\n",
    "            loss = agent.train_step(batch)\n",
    "            episode_loss += loss\n",
    "            num_updates += 1\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        steps += 1\n",
    "    \n",
    "    # Mise √† jour du r√©seau cible\n",
    "    if (episode + 1) % CONFIG['target_update_frequency'] == 0:\n",
    "        agent.update_target_network()\n",
    "    \n",
    "    # D√©croissance d'epsilon\n",
    "    epsilon = max(CONFIG['epsilon_end'], epsilon * CONFIG['epsilon_decay'])\n",
    "    \n",
    "    # Enregistrement des m√©triques\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_lengths.append(steps)\n",
    "    avg_loss = episode_loss / max(num_updates, 1)\n",
    "    episode_losses.append(avg_loss)\n",
    "    success = info.get('success', False)\n",
    "    episode_successes.append(1 if success else 0)\n",
    "    \n",
    "    # Logging vers TensorBoard\n",
    "    tracker.log_metrics({\n",
    "        'reward': episode_reward,\n",
    "        'episode_length': steps,\n",
    "        'loss': avg_loss,\n",
    "        'epsilon': current_epsilon,\n",
    "        'success': 1 if success else 0,\n",
    "        'curriculum_stage': curriculum.current_stage_idx,\n",
    "        'buffer_size': len(replay_buffer.buffer)\n",
    "    }, step=episode)\n",
    "    \n",
    "    # Sauvegarde du meilleur mod√®le\n",
    "    if episode_reward > best_reward:\n",
    "        best_reward = episode_reward\n",
    "        torch.save(agent.q_network.state_dict(), BEST_MODEL_PATH)\n",
    "    \n",
    "    # Sauvegarde p√©riodique\n",
    "    if (episode + 1) % CONFIG['checkpoint_interval'] == 0:\n",
    "        save_checkpoint(episode + 1, agent, agent.optimizer, best_reward,\n",
    "                       epsilon, curriculum, CHECKPOINT_PATH)\n",
    "    \n",
    "    # Affichage des progr√®s\n",
    "    if (episode + 1) % CONFIG['progress_interval'] == 0:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        avg_reward_100 = np.mean(episode_rewards[-100:])\n",
    "        avg_length_100 = np.mean(episode_lengths[-100:])\n",
    "        success_rate_100 = np.mean(episode_successes[-100:]) * 100\n",
    "        \n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"üìä √âpisode {episode + 1}/{CONFIG['num_episodes']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"   üí∞ Reward: {episode_reward:.2f}\")\n",
    "        print(f\"   üìà Moyenne 100 derniers: {avg_reward_100:.2f}\")\n",
    "        print(f\"   üéØ Meilleur reward: {best_reward:.2f}\")\n",
    "        print(f\"   üìè Longueur: {steps} pas\")\n",
    "        print(f\"   ‚úÖ Succ√®s: {'Oui' if success else 'Non'}\")\n",
    "        print(f\"   üìä Taux de succ√®s (100): {success_rate_100:.1f}%\")\n",
    "        print(f\"   üìâ Loss moyenne: {avg_loss:.4f}\")\n",
    "        print(f\"   üé≤ Epsilon: {current_epsilon:.4f}\")\n",
    "        print(f\"   üìö Stage: {curriculum.get_current_stage_name()}\")\n",
    "        print(f\"   üíæ Buffer: {len(replay_buffer.buffer):,}/{CONFIG['buffer_capacity']:,}\")\n",
    "        print(f\"   ‚è±Ô∏è  Temps: {elapsed_hours:.2f}h / {CONFIG['max_runtime_hours']}h\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Mini graphique de progression\n",
    "        if len(episode_rewards) >= 100:\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            \n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.plot(episode_rewards[-100:])\n",
    "            plt.title('Rewards (100 derniers)')\n",
    "            plt.xlabel('√âpisode')\n",
    "            plt.ylabel('Reward')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.plot(episode_losses[-100:])\n",
    "            plt.title('Loss (100 derniers)')\n",
    "            plt.xlabel('√âpisode')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.subplot(1, 3, 3)\n",
    "            success_window = np.convolve(episode_successes, np.ones(10)/10, mode='valid')\n",
    "            plt.plot(success_window[-90:] * 100)\n",
    "            plt.title('Taux de succ√®s (fen√™tre mobile 10)')\n",
    "            plt.xlabel('√âpisode')\n",
    "            plt.ylabel('Succ√®s (%)')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Sauvegarde finale\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üèÅ ENTRA√éNEMENT TERMIN√â!\")\n",
    "print(f\"{'='*60}\")\n",
    "save_checkpoint(CONFIG['num_episodes'], agent, agent.optimizer, best_reward,\n",
    "               epsilon, curriculum, CHECKPOINT_PATH)\n",
    "\n",
    "tracker.finish()\n",
    "\n",
    "print(f\"\\n‚úÖ Meilleur reward obtenu: {best_reward:.2f}\")\n",
    "print(f\"‚úÖ Taux de succ√®s final: {np.mean(episode_successes[-100:]) * 100:.1f}%\")\n",
    "print(f\"‚úÖ Dur√©e totale: {(time.time() - training_start_time) / 3600:.2f}h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8a35e6",
   "metadata": {},
   "source": [
    "## üìä √âtape 8: Visualisation des R√©sultats d'Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eca189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== VISUALISATION COMPL√àTE DES R√âSULTATS ====================\n",
    "\n",
    "print(\"üìä G√©n√©ration des visualisations...\")\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# 1. Courbe des rewards\n",
    "ax1 = plt.subplot(3, 2, 1)\n",
    "plt.plot(episode_rewards, alpha=0.6, label='Reward par √©pisode')\n",
    "# Moyenne mobile sur 50 √©pisodes\n",
    "if len(episode_rewards) >= 50:\n",
    "    ma_50 = np.convolve(episode_rewards, np.ones(50)/50, mode='valid')\n",
    "    plt.plot(range(49, len(episode_rewards)), ma_50, 'r-', linewidth=2, label='Moyenne mobile 50')\n",
    "plt.xlabel('√âpisode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('√âvolution des Rewards')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Courbe des losses\n",
    "ax2 = plt.subplot(3, 2, 2)\n",
    "plt.plot(episode_losses, alpha=0.6, label='Loss par √©pisode')\n",
    "if len(episode_losses) >= 50:\n",
    "    ma_loss_50 = np.convolve(episode_losses, np.ones(50)/50, mode='valid')\n",
    "    plt.plot(range(49, len(episode_losses)), ma_loss_50, 'r-', linewidth=2, label='Moyenne mobile 50')\n",
    "plt.xlabel('√âpisode')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('√âvolution de la Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Taux de succ√®s\n",
    "ax3 = plt.subplot(3, 2, 3)\n",
    "if len(episode_successes) >= 50:\n",
    "    success_ma = np.convolve(episode_successes, np.ones(50)/50, mode='valid') * 100\n",
    "    plt.plot(range(49, len(episode_successes)), success_ma, 'g-', linewidth=2)\n",
    "plt.xlabel('√âpisode')\n",
    "plt.ylabel('Taux de succ√®s (%)')\n",
    "plt.title('Taux de Succ√®s (Moyenne mobile 50)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Longueur des √©pisodes\n",
    "ax4 = plt.subplot(3, 2, 4)\n",
    "plt.plot(episode_lengths, alpha=0.6)\n",
    "if len(episode_lengths) >= 50:\n",
    "    ma_len_50 = np.convolve(episode_lengths, np.ones(50)/50, mode='valid')\n",
    "    plt.plot(range(49, len(episode_lengths)), ma_len_50, 'r-', linewidth=2)\n",
    "plt.xlabel('√âpisode')\n",
    "plt.ylabel('Nombre de pas')\n",
    "plt.title('Longueur des √âpisodes')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Distribution des rewards (100 derniers)\n",
    "ax5 = plt.subplot(3, 2, 5)\n",
    "plt.hist(episode_rewards[-100:], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Reward')\n",
    "plt.ylabel('Fr√©quence')\n",
    "plt.title('Distribution des Rewards (100 derniers √©pisodes)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Statistiques finales\n",
    "ax6 = plt.subplot(3, 2, 6)\n",
    "ax6.axis('off')\n",
    "stats_text = f\"\"\"\n",
    "STATISTIQUES FINALES\n",
    "\n",
    "Nombre d'√©pisodes: {len(episode_rewards)}\n",
    "Meilleur reward: {best_reward:.2f}\n",
    "\n",
    "Reward moyen (tout): {np.mean(episode_rewards):.2f}\n",
    "Reward moyen (100 derniers): {np.mean(episode_rewards[-100:]):.2f}\n",
    "\n",
    "Taux succ√®s (tout): {np.mean(episode_successes)*100:.1f}%\n",
    "Taux succ√®s (100 derniers): {np.mean(episode_successes[-100:])*100:.1f}%\n",
    "\n",
    "Loss finale: {episode_losses[-1]:.4f}\n",
    "Longueur moyenne: {np.mean(episode_lengths):.0f} pas\n",
    "\n",
    "Buffer final: {len(replay_buffer.buffer):,}\n",
    "Epsilon final: {epsilon:.4f}\n",
    "Stage final: {curriculum.get_current_stage_name()}\n",
    "\"\"\"\n",
    "ax6.text(0.1, 0.5, stats_text, fontsize=12, family='monospace',\n",
    "         verticalalignment='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(RESULTS_DIR) / 'training_results.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Graphique sauvegard√©: {Path(RESULTS_DIR) / 'training_results.png'}\")\n",
    "plt.show()\n",
    "\n",
    "# ==================== SAUVEGARDE DES DONN√âES ====================\n",
    "print(\"\\nüíæ Sauvegarde des donn√©es...\")\n",
    "\n",
    "# Sauvegarde CSV\n",
    "import pandas as pd\n",
    "\n",
    "df_results = pd.DataFrame({\n",
    "    'episode': range(len(episode_rewards)),\n",
    "    'reward': episode_rewards,\n",
    "    'length': episode_lengths,\n",
    "    'loss': episode_losses,\n",
    "    'success': episode_successes\n",
    "})\n",
    "\n",
    "csv_path = Path(RESULTS_DIR) / 'training_history.csv'\n",
    "df_results.to_csv(csv_path, index=False)\n",
    "print(f\"‚úÖ Historique sauvegard√©: {csv_path}\")\n",
    "\n",
    "# Sauvegarde JSON avec statistiques\n",
    "results_summary = {\n",
    "    'config': CONFIG,\n",
    "    'training_info': {\n",
    "        'num_episodes': len(episode_rewards),\n",
    "        'best_reward': float(best_reward),\n",
    "        'final_epsilon': float(epsilon),\n",
    "        'final_stage': curriculum.get_current_stage_name(),\n",
    "        'training_duration_hours': (time.time() - training_start_time) / 3600\n",
    "    },\n",
    "    'statistics': {\n",
    "        'mean_reward_all': float(np.mean(episode_rewards)),\n",
    "        'mean_reward_last_100': float(np.mean(episode_rewards[-100:])),\n",
    "        'std_reward_last_100': float(np.std(episode_rewards[-100:])),\n",
    "        'success_rate_all': float(np.mean(episode_successes)),\n",
    "        'success_rate_last_100': float(np.mean(episode_successes[-100:])),\n",
    "        'mean_episode_length': float(np.mean(episode_lengths)),\n",
    "        'final_buffer_size': len(replay_buffer.buffer)\n",
    "    }\n",
    "}\n",
    "\n",
    "json_path = Path(RESULTS_DIR) / 'training_summary.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "print(f\"‚úÖ R√©sum√© sauvegard√©: {json_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TOUTES LES VISUALISATIONS ET DONN√âES SAUVEGARD√âES!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54919c6c",
   "metadata": {},
   "source": [
    "## üß™ √âtape 9: √âvaluation du Mod√®le Entra√Æn√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b8034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== √âVALUATION SUR 50 √âPISODES DE TEST ====================\n",
    "\n",
    "print(\"üß™ √âvaluation du meilleur mod√®le...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Charger le meilleur mod√®le\n",
    "if BEST_MODEL_PATH.exists():\n",
    "    agent.q_network.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
    "    print(f\"‚úÖ Meilleur mod√®le charg√©: {BEST_MODEL_PATH}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Utilisation du mod√®le actuel (pas de meilleur mod√®le sauvegard√©)\")\n",
    "\n",
    "agent.q_network.eval()  # Mode √©valuation\n",
    "\n",
    "num_test_episodes = 50\n",
    "test_rewards = []\n",
    "test_lengths = []\n",
    "test_successes = []\n",
    "\n",
    "print(f\"\\nüéÆ Lancement de {num_test_episodes} √©pisodes de test...\")\n",
    "print(\"(Sans exploration - epsilon=0)\\n\")\n",
    "\n",
    "for test_ep in range(num_test_episodes):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done and steps < CONFIG['max_steps_per_episode']:\n",
    "        # Action sans exploration (epsilon=0)\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = agent.q_network(state_tensor)\n",
    "            action = q_values.argmax(dim=1).item()\n",
    "        \n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        steps += 1\n",
    "    \n",
    "    test_rewards.append(episode_reward)\n",
    "    test_lengths.append(steps)\n",
    "    success = info.get('success', False)\n",
    "    test_successes.append(1 if success else 0)\n",
    "    \n",
    "    if (test_ep + 1) % 10 == 0:\n",
    "        print(f\"Episode {test_ep + 1}/{num_test_episodes}: \"\n",
    "              f\"Reward={episode_reward:.2f}, \"\n",
    "              f\"Longueur={steps}, \"\n",
    "              f\"Succ√®s={'‚úÖ' if success else '‚ùå'}\")\n",
    "\n",
    "# ==================== STATISTIQUES D'√âVALUATION ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä R√âSULTATS DE L'√âVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Reward moyen: {np.mean(test_rewards):.2f} ¬± {np.std(test_rewards):.2f}\")\n",
    "print(f\"Reward min: {np.min(test_rewards):.2f}\")\n",
    "print(f\"Reward max: {np.max(test_rewards):.2f}\")\n",
    "print(f\"Longueur moyenne: {np.mean(test_lengths):.1f} pas\")\n",
    "print(f\"Taux de succ√®s: {np.mean(test_successes)*100:.1f}% ({sum(test_successes)}/{num_test_episodes})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ==================== VISUALISATION DES R√âSULTATS DE TEST ====================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Rewards de test\n",
    "axes[0].bar(range(num_test_episodes), test_rewards, alpha=0.7, color='skyblue')\n",
    "axes[0].axhline(y=np.mean(test_rewards), color='r', linestyle='--', label='Moyenne')\n",
    "axes[0].set_xlabel('√âpisode de test')\n",
    "axes[0].set_ylabel('Reward')\n",
    "axes[0].set_title('Rewards de Test')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution des rewards\n",
    "axes[1].hist(test_rewards, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[1].axvline(x=np.mean(test_rewards), color='r', linestyle='--', linewidth=2, label='Moyenne')\n",
    "axes[1].set_xlabel('Reward')\n",
    "axes[1].set_ylabel('Fr√©quence')\n",
    "axes[1].set_title('Distribution des Rewards de Test')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Succ√®s/√âchecs\n",
    "success_count = sum(test_successes)\n",
    "fail_count = num_test_episodes - success_count\n",
    "axes[2].pie([success_count, fail_count], \n",
    "           labels=['Succ√®s', '√âchecs'],\n",
    "           autopct='%1.1f%%',\n",
    "           colors=['lightgreen', 'lightcoral'],\n",
    "           startangle=90)\n",
    "axes[2].set_title('Taux de Succ√®s')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(RESULTS_DIR) / 'evaluation_results.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"\\n‚úÖ Visualisation sauvegard√©e: {Path(RESULTS_DIR) / 'evaluation_results.png'}\")\n",
    "plt.show()\n",
    "\n",
    "# Sauvegarde des r√©sultats de test\n",
    "test_results = {\n",
    "    'num_test_episodes': num_test_episodes,\n",
    "    'mean_reward': float(np.mean(test_rewards)),\n",
    "    'std_reward': float(np.std(test_rewards)),\n",
    "    'min_reward': float(np.min(test_rewards)),\n",
    "    'max_reward': float(np.max(test_rewards)),\n",
    "    'mean_length': float(np.mean(test_lengths)),\n",
    "    'success_rate': float(np.mean(test_successes)),\n",
    "    'num_successes': int(sum(test_successes))\n",
    "}\n",
    "\n",
    "test_results_path = Path(RESULTS_DIR) / 'evaluation_results.json'\n",
    "with open(test_results_path, 'w') as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "print(f\"‚úÖ R√©sultats de test sauvegard√©s: {test_results_path}\")\n",
    "\n",
    "agent.q_network.train()  # Retour en mode entra√Ænement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae5a9eb",
   "metadata": {},
   "source": [
    "## üìà √âtape 10: Lancement de TensorBoard (Optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eef3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== LANCEMENT DE TENSORBOARD ====================\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üìà Lancement de TensorBoard dans Colab...\")\n",
    "    \n",
    "    # Charger l'extension TensorBoard\n",
    "    %load_ext tensorboard\n",
    "    \n",
    "    # Lancer TensorBoard\n",
    "    %tensorboard --logdir {TENSORBOARD_DIR}\n",
    "    \n",
    "    print(f\"‚úÖ TensorBoard lanc√©!\")\n",
    "    print(f\"üìÅ Log directory: {TENSORBOARD_DIR}\")\n",
    "    print(\"\\n‚ÑπÔ∏è  TensorBoard s'affiche ci-dessus.\")\n",
    "    print(\"    Vous pouvez explorer les m√©triques, les graphiques et les scalaires.\")\n",
    "else:\n",
    "    print(\"üíª Mode local d√©tect√©\")\n",
    "    print(f\"Pour lancer TensorBoard, ex√©cutez dans un terminal:\")\n",
    "    print(f\"   tensorboard --logdir={TENSORBOARD_DIR}\")\n",
    "    print(f\"Puis ouvrez: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc832d40",
   "metadata": {},
   "source": [
    "## üìã R√©sum√© Final et Prochaines √âtapes\n",
    "\n",
    "### ‚úÖ Ce qui a √©t√© accompli:\n",
    "1. ‚úÖ Configuration GPU et montage Google Drive\n",
    "2. ‚úÖ Installation des d√©pendances\n",
    "3. ‚úÖ Entra√Ænement complet de 2000 √©pisodes avec curriculum learning\n",
    "4. ‚úÖ Sauvegarde automatique des checkpoints tous les 100 √©pisodes\n",
    "5. ‚úÖ G√©n√©ration des visualisations (courbes, statistiques)\n",
    "6. ‚úÖ √âvaluation du mod√®le sur 50 √©pisodes de test\n",
    "7. ‚úÖ Export des r√©sultats (CSV, JSON, graphiques)\n",
    "\n",
    "### üìÅ Fichiers g√©n√©r√©s:\n",
    "- **Checkpoints:**\n",
    "  - `checkpoint_colab.pt` - Dernier checkpoint (peut reprendre l'entra√Ænement)\n",
    "  - `best_model_colab.pt` - Meilleur mod√®le bas√© sur le reward\n",
    "  \n",
    "- **R√©sultats:**\n",
    "  - `training_history.csv` - Historique complet de l'entra√Ænement\n",
    "  - `training_summary.json` - Statistiques r√©sum√©es\n",
    "  - `evaluation_results.json` - R√©sultats des tests\n",
    "  - `training_results.png` - Graphiques d'entra√Ænement\n",
    "  - `evaluation_results.png` - Graphiques d'√©valuation\n",
    "\n",
    "### üöÄ Prochaines √©tapes possibles:\n",
    "\n",
    "#### 1. **Am√©liorer les performances:**\n",
    "   - Augmenter le nombre d'√©pisodes (3000-5000)\n",
    "   - Ajuster les hyperparam√®tres (learning rate, architecture)\n",
    "   - Modifier la fonction de r√©compense\n",
    "   - Ajouter plus d'obstacles ou changer leur configuration\n",
    "\n",
    "#### 2. **Exp√©rimenter avec d'autres algorithmes:**\n",
    "   - Essayer PPO, A3C, ou SAC\n",
    "   - Impl√©menter le Double DQN ou Dueling DQN\n",
    "   - Tester Rainbow DQN\n",
    "\n",
    "#### 3. **Analyser en d√©tail:**\n",
    "   - Visualiser les trajectoires de l'agent\n",
    "   - Analyser les Q-values\n",
    "   - Cr√©er des heatmaps de la politique apprise\n",
    "   - G√©n√©rer des vid√©os de l'agent en action\n",
    "\n",
    "#### 4. **D√©ployer le mod√®le:**\n",
    "   - Cr√©er une interface interactive\n",
    "   - D√©ployer sur un serveur web\n",
    "   - Optimiser pour l'inf√©rence (quantization, pruning)\n",
    "\n",
    "### üí° Pour relancer l'entra√Ænement:\n",
    "Si l'entra√Ænement s'interrompt (timeout Colab), **il suffit de relancer toutes les cellules**. Le syst√®me d√©tectera automatiquement le checkpoint et reprendra l√† o√π il s'√©tait arr√™t√©!\n",
    "\n",
    "### üìä Pour acc√©der aux r√©sultats:\n",
    "Les r√©sultats sont automatiquement sauvegard√©s dans votre Google Drive:\n",
    "- `/content/drive/MyDrive/RL_Project/results/`\n",
    "- `/content/drive/MyDrive/RL_Project/checkpoints/`\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ F√âLICITATIONS! Vous avez entra√Æn√© avec succ√®s un agent DQN sur Google Colab! üéâ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
