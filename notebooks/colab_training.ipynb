{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4f7d8e9",
   "metadata": {},
   "source": [
    "## üìã 1. Setup & Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d45778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check GPU\n",
    "gpu_info = !nvidia-smi\n",
    "print('\\n'.join(gpu_info))\n",
    "\n",
    "# Check CUDA\n",
    "import torch\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b070b",
   "metadata": {},
   "source": [
    "## üíæ 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cabdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directory in Drive\n",
    "PROJECT_DIR = Path('/content/drive/MyDrive/RL_Robot_Navigation')\n",
    "PROJECT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create subdirectories\n",
    "(PROJECT_DIR / 'checkpoints').mkdir(exist_ok=True)\n",
    "(PROJECT_DIR / 'results').mkdir(exist_ok=True)\n",
    "(PROJECT_DIR / 'logs').mkdir(exist_ok=True)\n",
    "(PROJECT_DIR / 'videos').mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Project directory: {PROJECT_DIR}\")\n",
    "print(f\"‚úÖ Subdirectories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e1493",
   "metadata": {},
   "source": [
    "## üì• 3. Clone Repository & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5573ccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (replace with your GitHub URL)\n",
    "REPO_URL = \"https://github.com/YOUR_USERNAME/robot_navigation_rl.git\"\n",
    "REPO_DIR = Path('/content/robot_navigation_rl')\n",
    "\n",
    "if REPO_DIR.exists():\n",
    "    print(\"üìÇ Repository already exists, pulling latest changes...\")\n",
    "    !cd {REPO_DIR} && git pull\n",
    "else:\n",
    "    print(\"üì• Cloning repository...\")\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir(REPO_DIR)\n",
    "print(f\"\\n‚úÖ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffe3dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q gymnasium numpy matplotlib pandas\n",
    "!pip install -q tensorboard wandb optuna\n",
    "!pip install -q tqdm psutil GPUtil\n",
    "!pip install -q imageio imageio-ffmpeg pillow\n",
    "\n",
    "# Install project in development mode\n",
    "!pip install -q -e .\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1783dd0d",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee655ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    # Environment\n",
    "    'env_size': (10, 10),\n",
    "    'num_obstacles': 5,\n",
    "    'num_goals': 3,\n",
    "    \n",
    "    # Agent\n",
    "    'algorithm': 'DQN',  # or 'RainbowDQN', 'AdaptiveDQN'\n",
    "    'hidden_dims': [128, 128],\n",
    "    'learning_rate': 1e-3,\n",
    "    'gamma': 0.99,\n",
    "    'batch_size': 64,\n",
    "    'buffer_size': 50000,\n",
    "    'target_update': 500,\n",
    "    \n",
    "    # Training\n",
    "    'num_episodes': 2000,\n",
    "    'max_steps': 200,\n",
    "    'epsilon_start': 1.0,\n",
    "    'epsilon_end': 0.01,\n",
    "    'epsilon_decay': 0.995,\n",
    "    \n",
    "    # Checkpointing\n",
    "    'checkpoint_interval': 100,\n",
    "    'save_best_only': True,\n",
    "    \n",
    "    # Paths (on Google Drive)\n",
    "    'checkpoint_dir': str(PROJECT_DIR / 'checkpoints'),\n",
    "    'results_dir': str(PROJECT_DIR / 'results'),\n",
    "    'log_dir': str(PROJECT_DIR / 'logs'),\n",
    "    'video_dir': str(PROJECT_DIR / 'videos'),\n",
    "    \n",
    "    # Resource management\n",
    "    'use_amp': True,  # Mixed precision\n",
    "    'num_workers': 2,  # Data loading\n",
    "    'prefetch_factor': 2,\n",
    "    \n",
    "    # Monitoring\n",
    "    'wandb_project': 'robot-navigation-colab',\n",
    "    'wandb_enabled': False,  # Set to True if using WandB\n",
    "    'tensorboard_enabled': True,\n",
    "}\n",
    "\n",
    "# Email notification (optional)\n",
    "NOTIFICATION_EMAIL = \"your_email@example.com\"  # Change this\n",
    "SEND_EMAIL_NOTIFICATION = False  # Set to True to enable\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba327319",
   "metadata": {},
   "source": [
    "## üîÑ 5. Auto-Resume Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b425cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def find_latest_checkpoint(checkpoint_dir):\n",
    "    \"\"\"Find the latest checkpoint in directory.\"\"\"\n",
    "    checkpoint_files = glob.glob(str(Path(checkpoint_dir) / '*.pt'))\n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    \n",
    "    # Sort by modification time\n",
    "    latest = max(checkpoint_files, key=os.path.getmtime)\n",
    "    return latest\n",
    "\n",
    "def load_training_state(checkpoint_path):\n",
    "    \"\"\"Load training state from checkpoint.\"\"\"\n",
    "    if checkpoint_path is None or not Path(checkpoint_path).exists():\n",
    "        return None\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    return checkpoint\n",
    "\n",
    "# Check for existing checkpoint\n",
    "latest_checkpoint = find_latest_checkpoint(config['checkpoint_dir'])\n",
    "if latest_checkpoint:\n",
    "    print(f\"‚úÖ Found checkpoint: {latest_checkpoint}\")\n",
    "    resume_training = input(\"Resume from this checkpoint? (y/n): \").lower() == 'y'\n",
    "    if resume_training:\n",
    "        config['resume_from'] = latest_checkpoint\n",
    "        print(\"üîÑ Will resume training from checkpoint\")\n",
    "    else:\n",
    "        config['resume_from'] = None\n",
    "        print(\"üÜï Starting fresh training\")\n",
    "else:\n",
    "    config['resume_from'] = None\n",
    "    print(\"üÜï No checkpoint found, starting fresh training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07774d83",
   "metadata": {},
   "source": [
    "## üèãÔ∏è 6. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed508c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, str(REPO_DIR))\n",
    "\n",
    "# Import project modules\n",
    "from src.agents.dqn_agent import DQNAgent\n",
    "from src.environment.robot_env import RobotNavigationEnv\n",
    "from src.training.trainer import Trainer\n",
    "from src.utils.logger import setup_logger\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nüñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "# Create environment\n",
    "env = RobotNavigationEnv(\n",
    "    size=config['env_size'],\n",
    "    num_obstacles=config['num_obstacles'],\n",
    "    num_goals=config['num_goals']\n",
    ")\n",
    "\n",
    "# Create agent\n",
    "agent = DQNAgent(\n",
    "    state_dim=env.observation_space.shape[0],\n",
    "    action_dim=env.action_space.n,\n",
    "    hidden_dims=config['hidden_dims'],\n",
    "    learning_rate=config['learning_rate'],\n",
    "    gamma=config['gamma'],\n",
    "    buffer_size=config['buffer_size'],\n",
    "    batch_size=config['batch_size'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Setup logger\n",
    "logger = setup_logger(\n",
    "    log_dir=config['log_dir'],\n",
    "    tensorboard=config['tensorboard_enabled']\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a935cf2f",
   "metadata": {},
   "source": [
    "## üöÄ 7. Training Loop with Auto-Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2468c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import signal\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Colab timeout handling\n",
    "COLAB_TIMEOUT = 12 * 3600  # 12 hours\n",
    "SAVE_BEFORE_TIMEOUT = 10 * 60  # Save 10 minutes before timeout\n",
    "start_time = time.time()\n",
    "\n",
    "def check_timeout():\n",
    "    \"\"\"Check if approaching Colab timeout.\"\"\"\n",
    "    elapsed = time.time() - start_time\n",
    "    return elapsed > (COLAB_TIMEOUT - SAVE_BEFORE_TIMEOUT)\n",
    "\n",
    "# Training metrics\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "success_rates = []\n",
    "losses = []\n",
    "best_reward = float('-inf')\n",
    "\n",
    "# Load checkpoint if resuming\n",
    "start_episode = 0\n",
    "if config['resume_from']:\n",
    "    checkpoint = load_training_state(config['resume_from'])\n",
    "    if checkpoint:\n",
    "        agent.load_state_dict(checkpoint['agent_state'])\n",
    "        start_episode = checkpoint['episode']\n",
    "        best_reward = checkpoint.get('best_reward', float('-inf'))\n",
    "        print(f\"‚úÖ Resumed from episode {start_episode}\")\n",
    "\n",
    "# Progress bar\n",
    "pbar = tqdm(range(start_episode, config['num_episodes']), desc=\"Training\")\n",
    "\n",
    "try:\n",
    "    for episode in pbar:\n",
    "        # Check for timeout\n",
    "        if check_timeout():\n",
    "            print(\"\\n‚ö†Ô∏è Approaching Colab timeout, saving checkpoint...\")\n",
    "            checkpoint_path = Path(config['checkpoint_dir']) / f'checkpoint_timeout_ep{episode}.pt'\n",
    "            torch.save({\n",
    "                'episode': episode,\n",
    "                'agent_state': agent.state_dict(),\n",
    "                'best_reward': best_reward,\n",
    "                'metrics': {\n",
    "                    'rewards': episode_rewards,\n",
    "                    'lengths': episode_lengths,\n",
    "                    'success_rates': success_rates,\n",
    "                    'losses': losses\n",
    "                }\n",
    "            }, checkpoint_path)\n",
    "            print(f\"üíæ Saved checkpoint: {checkpoint_path}\")\n",
    "            break\n",
    "        \n",
    "        # Training episode\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_loss = []\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        while not done and steps < config['max_steps']:\n",
    "            # Select action\n",
    "            epsilon = max(\n",
    "                config['epsilon_end'],\n",
    "                config['epsilon_start'] * (config['epsilon_decay'] ** episode)\n",
    "            )\n",
    "            action = agent.select_action(state, epsilon)\n",
    "            \n",
    "            # Take step\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store transition\n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Train agent\n",
    "            if agent.can_train():\n",
    "                loss = agent.train_step()\n",
    "                episode_loss.append(loss)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        \n",
    "        # Update target network\n",
    "        if episode % config['target_update'] == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        # Record metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        if episode_loss:\n",
    "            losses.append(np.mean(episode_loss))\n",
    "        \n",
    "        # Calculate success rate (last 100 episodes)\n",
    "        recent_rewards = episode_rewards[-100:]\n",
    "        success_rate = sum(r > 0 for r in recent_rewards) / len(recent_rewards)\n",
    "        success_rates.append(success_rate)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'reward': f'{episode_reward:.2f}',\n",
    "            'success': f'{success_rate:.2%}',\n",
    "            'epsilon': f'{epsilon:.3f}',\n",
    "            'steps': steps\n",
    "        })\n",
    "        \n",
    "        # Log to TensorBoard\n",
    "        if config['tensorboard_enabled']:\n",
    "            logger.log_scalar('reward', episode_reward, episode)\n",
    "            logger.log_scalar('success_rate', success_rate, episode)\n",
    "            logger.log_scalar('epsilon', epsilon, episode)\n",
    "            if episode_loss:\n",
    "                logger.log_scalar('loss', np.mean(episode_loss), episode)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if episode % config['checkpoint_interval'] == 0:\n",
    "            if config['save_best_only']:\n",
    "                if episode_reward > best_reward:\n",
    "                    best_reward = episode_reward\n",
    "                    checkpoint_path = Path(config['checkpoint_dir']) / 'best_model.pt'\n",
    "                    torch.save({\n",
    "                        'episode': episode,\n",
    "                        'agent_state': agent.state_dict(),\n",
    "                        'best_reward': best_reward,\n",
    "                        'metrics': {\n",
    "                            'rewards': episode_rewards,\n",
    "                            'lengths': episode_lengths,\n",
    "                            'success_rates': success_rates,\n",
    "                            'losses': losses\n",
    "                        }\n",
    "                    }, checkpoint_path)\n",
    "            else:\n",
    "                checkpoint_path = Path(config['checkpoint_dir']) / f'checkpoint_ep{episode}.pt'\n",
    "                torch.save({\n",
    "                    'episode': episode,\n",
    "                    'agent_state': agent.state_dict(),\n",
    "                    'best_reward': best_reward,\n",
    "                    'metrics': {\n",
    "                        'rewards': episode_rewards,\n",
    "                        'lengths': episode_lengths,\n",
    "                        'success_rates': success_rates,\n",
    "                        'losses': losses\n",
    "                    }\n",
    "                }, checkpoint_path)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è Training interrupted, saving checkpoint...\")\n",
    "    checkpoint_path = Path(config['checkpoint_dir']) / f'checkpoint_interrupted_ep{episode}.pt'\n",
    "    torch.save({\n",
    "        'episode': episode,\n",
    "        'agent_state': agent.state_dict(),\n",
    "        'best_reward': best_reward,\n",
    "        'metrics': {\n",
    "            'rewards': episode_rewards,\n",
    "            'lengths': episode_lengths,\n",
    "            'success_rates': success_rates,\n",
    "            'losses': losses\n",
    "        }\n",
    "    }, checkpoint_path)\n",
    "    print(f\"üíæ Saved checkpoint: {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "print(f\"üìä Final metrics:\")\n",
    "print(f\"  - Episodes trained: {len(episode_rewards)}\")\n",
    "print(f\"  - Best reward: {best_reward:.2f}\")\n",
    "print(f\"  - Final success rate: {success_rates[-1]:.2%}\")\n",
    "print(f\"  - Average reward (last 100): {np.mean(episode_rewards[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61df7067",
   "metadata": {},
   "source": [
    "## üìä 8. Quick Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d154a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Training Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Episode Rewards\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(episode_rewards, alpha=0.3, label='Raw')\n",
    "if len(episode_rewards) > 50:\n",
    "    moving_avg = np.convolve(episode_rewards, np.ones(50)/50, mode='valid')\n",
    "    ax1.plot(range(49, len(episode_rewards)), moving_avg, linewidth=2, label='MA(50)')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.set_title('Episode Rewards')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Success Rate\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(success_rates, linewidth=2, color='green')\n",
    "ax2.axhline(y=0.7, color='r', linestyle='--', label='70% Target')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Success Rate')\n",
    "ax2.set_title('Success Rate (Last 100 Episodes)')\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Episode Lengths\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(episode_lengths, alpha=0.5)\n",
    "if len(episode_lengths) > 50:\n",
    "    moving_avg = np.convolve(episode_lengths, np.ones(50)/50, mode='valid')\n",
    "    ax3.plot(range(49, len(episode_lengths)), moving_avg, linewidth=2, color='orange')\n",
    "ax3.set_xlabel('Episode')\n",
    "ax3.set_ylabel('Steps')\n",
    "ax3.set_title('Episode Lengths')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Loss\n",
    "ax4 = axes[1, 1]\n",
    "if losses:\n",
    "    ax4.plot(losses, alpha=0.5)\n",
    "    if len(losses) > 50:\n",
    "        moving_avg = np.convolve(losses, np.ones(50)/50, mode='valid')\n",
    "        ax4.plot(range(49, len(losses)), moving_avg, linewidth=2, color='red')\n",
    "    ax4.set_xlabel('Episode')\n",
    "    ax4.set_ylabel('Loss')\n",
    "    ax4.set_title('Training Loss')\n",
    "    ax4.set_yscale('log')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(PROJECT_DIR / 'results' / 'training_summary.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüíæ Plot saved to:\", PROJECT_DIR / 'results' / 'training_summary.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d56142f",
   "metadata": {},
   "source": [
    "## üìß 9. Email Notification (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f47d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEND_EMAIL_NOTIFICATION:\n",
    "    try:\n",
    "        import smtplib\n",
    "        from email.mime.text import MIMEText\n",
    "        from email.mime.multipart import MIMEMultipart\n",
    "        \n",
    "        # Email content\n",
    "        subject = \"üöÄ RL Training Complete - Google Colab\"\n",
    "        body = f\"\"\"\n",
    "        Training completed successfully!\n",
    "        \n",
    "        üìä Summary:\n",
    "        - Episodes: {len(episode_rewards)}\n",
    "        - Best Reward: {best_reward:.2f}\n",
    "        - Final Success Rate: {success_rates[-1]:.2%}\n",
    "        - Average Reward (last 100): {np.mean(episode_rewards[-100:]):.2f}\n",
    "        \n",
    "        üìÅ Results saved to: {PROJECT_DIR}\n",
    "        \n",
    "        Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Note: You'll need to configure SMTP settings\n",
    "        # For Gmail, you may need an app password\n",
    "        # This is a placeholder - configure with your SMTP server\n",
    "        print(\"üìß Email notification:\")\n",
    "        print(subject)\n",
    "        print(body)\n",
    "        print(\"\\n‚ö†Ô∏è Configure SMTP settings to enable actual email sending\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not send email: {e}\")\n",
    "else:\n",
    "    print(\"üìß Email notification disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f16425",
   "metadata": {},
   "source": [
    "## üíæ 10. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de36c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Save metrics to JSON\n",
    "metrics = {\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'config': config,\n",
    "    'episodes_trained': len(episode_rewards),\n",
    "    'best_reward': float(best_reward),\n",
    "    'final_success_rate': float(success_rates[-1]) if success_rates else 0.0,\n",
    "    'episode_rewards': [float(r) for r in episode_rewards],\n",
    "    'episode_lengths': [int(l) for l in episode_lengths],\n",
    "    'success_rates': [float(sr) for sr in success_rates],\n",
    "    'losses': [float(l) for l in losses] if losses else []\n",
    "}\n",
    "\n",
    "metrics_path = PROJECT_DIR / 'results' / f'metrics_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Metrics saved to: {metrics_path}\")\n",
    "\n",
    "# Save final model\n",
    "final_model_path = PROJECT_DIR / 'checkpoints' / 'final_model.pt'\n",
    "torch.save({\n",
    "    'episode': len(episode_rewards),\n",
    "    'agent_state': agent.state_dict(),\n",
    "    'best_reward': best_reward,\n",
    "    'config': config\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"‚úÖ Final model saved to: {final_model_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ ALL DONE! Training completed successfully.\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìÅ All results saved to Google Drive: {PROJECT_DIR}\")\n",
    "print(\"\\nüí° Next steps:\")\n",
    "print(\"   1. Download results from Google Drive\")\n",
    "print(\"   2. Run visualizations locally\")\n",
    "print(\"   3. Evaluate model on test scenarios\")\n",
    "print(\"   4. Resume training if needed (checkpoint saved)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca0780",
   "metadata": {},
   "source": [
    "## üé¨ 11. Test Trained Agent (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cee028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained agent\n",
    "print(\"üß™ Testing trained agent...\\n\")\n",
    "\n",
    "test_episodes = 10\n",
    "test_rewards = []\n",
    "test_successes = []\n",
    "\n",
    "for ep in range(test_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    while not done and steps < config['max_steps']:\n",
    "        # Use greedy policy (epsilon=0)\n",
    "        action = agent.select_action(state, epsilon=0.0)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "    \n",
    "    test_rewards.append(episode_reward)\n",
    "    test_successes.append(episode_reward > 0)\n",
    "    \n",
    "    print(f\"  Episode {ep+1}: Reward={episode_reward:.2f}, Steps={steps}, Success={episode_reward > 0}\")\n",
    "\n",
    "print(f\"\\nüìä Test Results:\")\n",
    "print(f\"  - Average Reward: {np.mean(test_rewards):.2f} ¬± {np.std(test_rewards):.2f}\")\n",
    "print(f\"  - Success Rate: {sum(test_successes)/len(test_successes):.2%}\")\n",
    "print(f\"  - Best Reward: {max(test_rewards):.2f}\")\n",
    "print(f\"  - Worst Reward: {min(test_rewards):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
