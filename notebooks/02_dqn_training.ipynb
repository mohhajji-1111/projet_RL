{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213bd30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from src.agents import DQNAgent\n",
    "from src.environment import NavigationEnv\n",
    "from src.training import BasicTrainer\n",
    "from src.utils import TrainingLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62d36f8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fd54c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7320011",
   "metadata": {},
   "source": [
    "## Create Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d2272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NavigationEnv()\n",
    "\n",
    "agent = DQNAgent(\n",
    "    state_dim=8,\n",
    "    action_dim=4,\n",
    "    learning_rate=1e-4,\n",
    "    gamma=0.99,\n",
    "    hidden_dims=[256, 256],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Agent created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2667e6c",
   "metadata": {},
   "source": [
    "## Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd45c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BasicTrainer(\n",
    "    agent=agent,\n",
    "    env=env,\n",
    "    num_episodes=500,\n",
    "    batch_size=64,\n",
    "    learning_starts=1000,\n",
    "    save_path='../trained_models/basic',\n",
    "    log_interval=10\n",
    ")\n",
    "\n",
    "# Create save directory\n",
    "Path('../trained_models/basic').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "stats = trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Mean reward (last 100): {stats['mean_reward']:.2f}\")\n",
    "print(f\"Mean length (last 100): {stats['mean_length']:.1f}\")\n",
    "print(f\"Training time: {stats['training_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e0496",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac4833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Episode rewards\n",
    "axes[0, 0].plot(stats['episode_rewards'], alpha=0.3)\n",
    "window = 50\n",
    "if len(stats['episode_rewards']) > window:\n",
    "    moving_avg = np.convolve(stats['episode_rewards'], \n",
    "                              np.ones(window)/window, mode='valid')\n",
    "    axes[0, 0].plot(range(window-1, len(stats['episode_rewards'])), \n",
    "                     moving_avg, linewidth=2, label=f'{window}-episode avg')\n",
    "    axes[0, 0].legend()\n",
    "axes[0, 0].set_title('Episode Rewards')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Reward')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Episode lengths\n",
    "axes[0, 1].plot(stats['episode_lengths'], alpha=0.5)\n",
    "axes[0, 1].set_title('Episode Lengths')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Steps')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Training losses\n",
    "if trainer.losses:\n",
    "    axes[1, 0].plot(trainer.losses, alpha=0.5)\n",
    "    axes[1, 0].set_title('Training Loss')\n",
    "    axes[1, 0].set_xlabel('Training Step')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-values\n",
    "if trainer.q_values:\n",
    "    axes[1, 1].plot(trainer.q_values, alpha=0.5)\n",
    "    axes[1, 1].set_title('Average Q-Values')\n",
    "    axes[1, 1].set_xlabel('Training Step')\n",
    "    axes[1, 1].set_ylabel('Q-Value')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/dqn_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef95765f",
   "metadata": {},
   "source": [
    "## Test Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a168945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for 10 episodes\n",
    "test_rewards = []\n",
    "\n",
    "for episode in range(10):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(1000):\n",
    "        action = agent.select_action(state, epsilon=0.0)  # Greedy\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    test_rewards.append(episode_reward)\n",
    "    print(f\"Test Episode {episode + 1}: Reward = {episode_reward:.2f}\")\n",
    "\n",
    "print(f\"\\nAverage test reward: {np.mean(test_rewards):.2f} Â± {np.std(test_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7723f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
