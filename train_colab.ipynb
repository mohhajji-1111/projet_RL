{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7257cee",
   "metadata": {},
   "source": [
    "## üì¶ 1. Installation et Setup\n",
    "\n",
    "Installation des d√©pendances n√©cessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b3da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des packages requis\n",
    "!pip install -q torch torchvision gymnasium pygame numpy matplotlib seaborn pyyaml tqdm scipy pandas tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee828cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier GPU disponible\n",
    "import torch\n",
    "print(f\"üî• CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   M√©moire: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  CPU mode (sera plus lent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b13107",
   "metadata": {},
   "source": [
    "## üìÅ 2. Cloner le Projet\n",
    "\n",
    "Clone ton repository GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439133f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloner le projet depuis GitHub\n",
    "!git clone https://github.com/mohhajji-1111/projet_RL.git\n",
    "%cd projet_RL\n",
    "\n",
    "print(\"‚úÖ Projet clon√© avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c3242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier la structure du projet\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfbf840",
   "metadata": {},
   "source": [
    "## üîß 3. Configuration de l'Environnement\n",
    "\n",
    "Setup de l'environnement Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bdffbc",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Important**: Cette cellule configure le Python path pour que les imports fonctionnent correctement.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2261e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ajouter le projet au Python path\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# V√©rifier que les modules sont accessibles\n",
    "print(f\"‚úÖ Project root: {project_root}\")\n",
    "print(f\"‚úÖ Python path configur√©\")\n",
    "print(f\"\\nüìÅ Structure du projet:\")\n",
    "!ls -la src/\n",
    "!ls -la src/agents/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041f80b3",
   "metadata": {},
   "source": [
    "## üéÆ 4. Test de l'Environnement\n",
    "\n",
    "V√©rifier que l'environnement fonctionne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcd2a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.environment.navigation_env import NavigationEnv\n",
    "import numpy as np\n",
    "\n",
    "# Cr√©er environnement\n",
    "env = NavigationEnv(width=800, height=600, render_mode=None)\n",
    "\n",
    "print(f\"‚úÖ Environnement cr√©√© avec succ√®s!\")\n",
    "print(f\"   Observation space: {env.observation_space.shape}\")\n",
    "print(f\"   Action space: {env.action_space.n} actions\")\n",
    "\n",
    "# Test rapide\n",
    "state, _ = env.reset()\n",
    "print(f\"\\nüìä √âtat initial: {state}\")\n",
    "\n",
    "# Faire quelques steps\n",
    "for i in range(5):\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    print(f\"   Step {i+1}: action={action}, reward={reward:.2f}, done={done}\")\n",
    "\n",
    "print(\"\\n‚úÖ Environnement fonctionne correctement!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289248db",
   "metadata": {},
   "source": [
    "## üß† 5. Initialiser l'Agent avec Curiosit√©\n",
    "\n",
    "Cr√©er l'agent CuriosityAgent avec le module ICM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a01c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agents.curiosity_agent import CuriosityAgent\n",
    "import yaml\n",
    "\n",
    "# Charger la configuration\n",
    "with open('configs/curiosity_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Cr√©er l'agent\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "agent = CuriosityAgent(\n",
    "    state_dim=env.observation_space.shape[0],\n",
    "    action_dim=env.action_space.n,\n",
    "    config=config['agent'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Agent cr√©√© avec succ√®s!\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   State dim: {env.observation_space.shape[0]}\")\n",
    "print(f\"   Action dim: {env.action_space.n}\")\n",
    "print(f\"   Feature dim: {config['agent']['feature_dim']}\")\n",
    "print(f\"   Curiosity beta: {config['agent']['curiosity_beta']}\")\n",
    "\n",
    "# Afficher les r√©seaux\n",
    "print(\"\\nüìä Architecture des r√©seaux ICM:\")\n",
    "print(f\"   Feature Network: {sum(p.numel() for p in agent.feature_net.parameters())} params\")\n",
    "print(f\"   Inverse Model: {sum(p.numel() for p in agent.inverse_model.parameters())} params\")\n",
    "print(f\"   Forward Model: {sum(p.numel() for p in agent.forward_model.parameters())} params\")\n",
    "print(f\"   Q-Network: {sum(p.numel() for p in agent.q_network.parameters())} params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e9832a",
   "metadata": {},
   "source": [
    "## üöÄ 6. Entra√Ænement de l'Agent\n",
    "\n",
    "Lancer l'entra√Ænement complet avec suivi en temps r√©el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3679650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import deque\n",
    "\n",
    "# Param√®tres d'entra√Ænement\n",
    "num_episodes = 1500\n",
    "eval_interval = 50\n",
    "save_interval = 100\n",
    "\n",
    "# Cr√©er dossiers pour sauvegardes\n",
    "os.makedirs('results/models/curiosity', exist_ok=True)\n",
    "os.makedirs('results/plots', exist_ok=True)\n",
    "os.makedirs('results/logs', exist_ok=True)\n",
    "\n",
    "# M√©triques\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "intrinsic_rewards = []\n",
    "success_rate = deque(maxlen=100)\n",
    "avg_rewards = deque(maxlen=100)\n",
    "\n",
    "# ICM losses\n",
    "forward_losses = []\n",
    "inverse_losses = []\n",
    "\n",
    "best_reward = -float('inf')\n",
    "\n",
    "print(\"üöÄ D√©but de l'entra√Ænement...\\n\")\n",
    "\n",
    "# Barre de progression\n",
    "pbar = tqdm(range(num_episodes), desc=\"Training\")\n",
    "\n",
    "for episode in pbar:\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_intrinsic = 0\n",
    "    episode_length = 0\n",
    "    done = False\n",
    "    truncated = False\n",
    "    \n",
    "    while not (done or truncated):\n",
    "        # S√©lectionner action\n",
    "        action = agent.select_action(state)\n",
    "        \n",
    "        # Step dans l'environnement\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        # Calculer r√©compense intrins√®que\n",
    "        intrinsic_reward = agent.compute_intrinsic_reward(\n",
    "            torch.FloatTensor(state).unsqueeze(0).to(device),\n",
    "            torch.LongTensor([action]).to(device),\n",
    "            torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "        )\n",
    "        \n",
    "        # Stocker transition\n",
    "        agent.store_transition(state, action, reward, next_state, done or truncated)\n",
    "        \n",
    "        # Entra√Æner\n",
    "        if len(agent.replay_buffer) > agent.batch_size:\n",
    "            loss = agent.train_step()\n",
    "        \n",
    "        episode_reward += reward\n",
    "        episode_intrinsic += intrinsic_reward\n",
    "        episode_length += 1\n",
    "        state = next_state\n",
    "    \n",
    "    # Enregistrer m√©triques\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_lengths.append(episode_length)\n",
    "    intrinsic_rewards.append(episode_intrinsic)\n",
    "    avg_rewards.append(episode_reward)\n",
    "    success_rate.append(1.0 if info.get('success', False) else 0.0)\n",
    "    \n",
    "    # ICM stats\n",
    "    icm_stats = agent.get_icm_stats()\n",
    "    forward_losses.append(icm_stats['forward_loss'])\n",
    "    inverse_losses.append(icm_stats['inverse_loss'])\n",
    "    \n",
    "    # Update progress bar\n",
    "    pbar.set_postfix({\n",
    "        'Reward': f\"{episode_reward:.2f}\",\n",
    "        'Avg': f\"{np.mean(avg_rewards):.2f}\",\n",
    "        'Success': f\"{np.mean(success_rate):.2%}\",\n",
    "        'Epsilon': f\"{agent.epsilon:.3f}\"\n",
    "    })\n",
    "    \n",
    "    # Sauvegarder meilleur mod√®le\n",
    "    if episode_reward > best_reward:\n",
    "        best_reward = episode_reward\n",
    "        agent.save_checkpoint('results/models/curiosity/best.pth')\n",
    "    \n",
    "    # Sauvegardes r√©guli√®res\n",
    "    if (episode + 1) % save_interval == 0:\n",
    "        agent.save_checkpoint(f'results/models/curiosity/checkpoint_{episode+1}.pth')\n",
    "        print(f\"\\nüíæ Checkpoint sauvegard√©: episode {episode+1}\")\n",
    "\n",
    "print(\"\\n‚úÖ Entra√Ænement termin√©!\")\n",
    "print(f\"   Meilleure r√©compense: {best_reward:.2f}\")\n",
    "print(f\"   Taux de succ√®s final: {np.mean(success_rate):.2%}\")\n",
    "print(f\"   R√©compense moyenne (100 derniers): {np.mean(avg_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0593905",
   "metadata": {},
   "source": [
    "## üìä 7. Visualisation des R√©sultats\n",
    "\n",
    "Analyse des performances de l'agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab357784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (16, 12)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. R√©compenses d'√©pisode\n",
    "ax = axes[0, 0]\n",
    "ax.plot(episode_rewards, alpha=0.3, label='Raw', color='blue')\n",
    "smoothed = gaussian_filter1d(episode_rewards, sigma=20)\n",
    "ax.plot(smoothed, label='Smoothed (œÉ=20)', color='red', linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Total Reward')\n",
    "ax.set_title('üìà R√©compenses d\\'√âpisode')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Moyenne mobile\n",
    "ax = axes[0, 1]\n",
    "window = 100\n",
    "moving_avg = [np.mean(episode_rewards[max(0, i-window):i+1]) for i in range(len(episode_rewards))]\n",
    "ax.plot(moving_avg, color='green', linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Average Reward')\n",
    "ax.set_title(f'üìä Moyenne Mobile (window={window})')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. R√©compenses intrins√®ques\n",
    "ax = axes[1, 0]\n",
    "ax.plot(intrinsic_rewards, alpha=0.4, color='orange')\n",
    "smoothed_int = gaussian_filter1d(intrinsic_rewards, sigma=20)\n",
    "ax.plot(smoothed_int, color='red', linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Intrinsic Reward')\n",
    "ax.set_title('üîç R√©compenses Intrins√®ques (Curiosit√©)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. ICM Losses\n",
    "ax = axes[1, 1]\n",
    "ax.plot(gaussian_filter1d(forward_losses, sigma=20), label='Forward Loss', color='purple', linewidth=2)\n",
    "ax.plot(gaussian_filter1d(inverse_losses, sigma=20), label='Inverse Loss', color='brown', linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('üß† ICM Losses')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Longueur des √©pisodes\n",
    "ax = axes[2, 0]\n",
    "ax.plot(episode_lengths, alpha=0.3, color='cyan')\n",
    "smoothed_len = gaussian_filter1d(episode_lengths, sigma=20)\n",
    "ax.plot(smoothed_len, color='darkblue', linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Steps')\n",
    "ax.set_title('‚è±Ô∏è Longueur des √âpisodes')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Distribution des r√©compenses\n",
    "ax = axes[2, 1]\n",
    "ax.hist(episode_rewards, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "ax.axvline(np.mean(episode_rewards), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(episode_rewards):.2f}')\n",
    "ax.axvline(np.median(episode_rewards), color='blue', linestyle='--', linewidth=2, label=f'Median: {np.median(episode_rewards):.2f}')\n",
    "ax.set_xlabel('Reward')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('üìä Distribution des R√©compenses')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/plots/training_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualisations sauvegard√©es dans results/plots/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a118b81d",
   "metadata": {},
   "source": [
    "## üìà 8. Statistiques D√©taill√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a81049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques finales\n",
    "print(\"=\"*60)\n",
    "print(\"üìä STATISTIQUES FINALES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüéØ R√©compenses:\")\n",
    "print(f\"   Moyenne totale: {np.mean(episode_rewards):.2f} ¬± {np.std(episode_rewards):.2f}\")\n",
    "print(f\"   M√©diane: {np.median(episode_rewards):.2f}\")\n",
    "print(f\"   Minimum: {np.min(episode_rewards):.2f}\")\n",
    "print(f\"   Maximum: {np.max(episode_rewards):.2f}\")\n",
    "print(f\"   Meilleur: {best_reward:.2f}\")\n",
    "\n",
    "# Derniers 100 √©pisodes\n",
    "last_100 = episode_rewards[-100:]\n",
    "print(f\"\\nüìä Performance (100 derniers √©pisodes):\")\n",
    "print(f\"   Moyenne: {np.mean(last_100):.2f} ¬± {np.std(last_100):.2f}\")\n",
    "print(f\"   Taux de succ√®s: {np.mean(list(success_rate)):.2%}\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è √âpisodes:\")\n",
    "print(f\"   Longueur moyenne: {np.mean(episode_lengths):.1f} steps\")\n",
    "print(f\"   Longueur m√©diane: {np.median(episode_lengths):.1f} steps\")\n",
    "\n",
    "print(f\"\\nüîç Curiosit√© (ICM):\")\n",
    "print(f\"   R√©compense intrins√®que moyenne: {np.mean(intrinsic_rewards):.4f}\")\n",
    "print(f\"   Forward loss finale: {forward_losses[-1]:.4f}\")\n",
    "print(f\"   Inverse loss finale: {inverse_losses[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nüß† Agent:\")\n",
    "print(f\"   Epsilon final: {agent.epsilon:.4f}\")\n",
    "print(f\"   Replay buffer size: {len(agent.replay_buffer)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb9eac9",
   "metadata": {},
   "source": [
    "## üíæ 9. T√©l√©charger les Mod√®les\n",
    "\n",
    "T√©l√©charger les mod√®les entra√Æn√©s vers Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ff818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monter Google Drive (optionnel)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copier les r√©sultats vers Drive\n",
    "!cp -r results /content/drive/MyDrive/projet_RL_results\n",
    "\n",
    "print(\"‚úÖ R√©sultats copi√©s vers Google Drive: MyDrive/projet_RL_results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ecb448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ou t√©l√©charger directement\n",
    "from google.colab import files\n",
    "\n",
    "# Compresser les r√©sultats\n",
    "!zip -r results.zip results/\n",
    "\n",
    "# T√©l√©charger\n",
    "files.download('results.zip')\n",
    "\n",
    "print(\"‚úÖ Archive results.zip t√©l√©charg√©e!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a14a945",
   "metadata": {},
   "source": [
    "## üéÆ 10. Test de l'Agent Entra√Æn√©\n",
    "\n",
    "Tester l'agent sur quelques √©pisodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le meilleur mod√®le\n",
    "agent.load_checkpoint('results/models/curiosity/best.pth')\n",
    "agent.epsilon = 0.0  # Mode exploitation pur\n",
    "\n",
    "print(\"üéÆ Test de l'agent entra√Æn√© (10 √©pisodes)...\\n\")\n",
    "\n",
    "test_rewards = []\n",
    "test_lengths = []\n",
    "\n",
    "for ep in range(10):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    truncated = False\n",
    "    \n",
    "    while not (done or truncated) and steps < 500:\n",
    "        action = agent.select_action(state)\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        steps += 1\n",
    "    \n",
    "    test_rewards.append(episode_reward)\n",
    "    test_lengths.append(steps)\n",
    "    success = \"‚úÖ\" if info.get('success', False) else \"‚ùå\"\n",
    "    print(f\"Episode {ep+1}: {success} Reward={episode_reward:.2f}, Steps={steps}\")\n",
    "\n",
    "print(f\"\\nüìä R√©sultats du test:\")\n",
    "print(f\"   Moyenne: {np.mean(test_rewards):.2f} ¬± {np.std(test_rewards):.2f}\")\n",
    "print(f\"   M√©diane: {np.median(test_rewards):.2f}\")\n",
    "print(f\"   Longueur moyenne: {np.mean(test_lengths):.1f} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe6d93a",
   "metadata": {},
   "source": [
    "## üìù 11. Sauvegarder le Rapport Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc7ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un rapport texte\n",
    "report = f\"\"\"\n",
    "RAPPORT D'ENTRA√éNEMENT - CURIOSITY AGENT\n",
    "{'='*60}\n",
    "\n",
    "Configuration:\n",
    "- √âpisodes: {num_episodes}\n",
    "- Device: {device}\n",
    "- Feature dim: {config['agent']['feature_dim']}\n",
    "- Curiosity beta: {config['agent']['curiosity_beta']}\n",
    "\n",
    "R√©sultats:\n",
    "- Meilleure r√©compense: {best_reward:.2f}\n",
    "- R√©compense moyenne: {np.mean(episode_rewards):.2f} ¬± {np.std(episode_rewards):.2f}\n",
    "- R√©compense m√©diane: {np.median(episode_rewards):.2f}\n",
    "- Taux de succ√®s (final): {np.mean(list(success_rate)):.2%}\n",
    "\n",
    "Performance (100 derniers √©pisodes):\n",
    "- R√©compense moyenne: {np.mean(last_100):.2f} ¬± {np.std(last_100):.2f}\n",
    "- Longueur moyenne: {np.mean(episode_lengths[-100:]):.1f} steps\n",
    "\n",
    "Curiosit√© (ICM):\n",
    "- R√©compense intrins√®que moyenne: {np.mean(intrinsic_rewards):.4f}\n",
    "- Forward loss finale: {forward_losses[-1]:.4f}\n",
    "- Inverse loss finale: {inverse_losses[-1]:.4f}\n",
    "\n",
    "Test (10 √©pisodes):\n",
    "- Moyenne: {np.mean(test_rewards):.2f} ¬± {np.std(test_rewards):.2f}\n",
    "- M√©diane: {np.median(test_rewards):.2f}\n",
    "- Longueur moyenne: {np.mean(test_lengths):.1f} steps\n",
    "\n",
    "{'='*60}\n",
    "\"\"\"\n",
    "\n",
    "# Sauvegarder\n",
    "with open('results/training_report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(report)\n",
    "print(\"‚úÖ Rapport sauvegard√©: results/training_report.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603301b1",
   "metadata": {},
   "source": [
    "## üéâ Fin de l'Entra√Ænement!\n",
    "\n",
    "### Fichiers g√©n√©r√©s:\n",
    "- `results/models/curiosity/best.pth` - Meilleur mod√®le\n",
    "- `results/models/curiosity/checkpoint_*.pth` - Checkpoints\n",
    "- `results/plots/training_summary.png` - Visualisations\n",
    "- `results/training_report.txt` - Rapport d√©taill√©\n",
    "\n",
    "### Prochaines √©tapes:\n",
    "1. T√©l√©charger les mod√®les entra√Æn√©s\n",
    "2. Tester localement avec l'interface GUI\n",
    "3. Comparer avec DQN baseline\n",
    "4. Optimiser les hyperparam√®tres si n√©cessaire\n",
    "\n",
    "**Bon courage! üöÄü§ñ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
